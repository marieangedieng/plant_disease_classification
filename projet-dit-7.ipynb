{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.9.9-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.1.2)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (77.0.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.3-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.74.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.11.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (108 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Collecting array_record>=0.5.0 (from tensorflow_datasets)\n",
      "  Downloading array_record-0.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (899 bytes)\n",
      "Collecting dm-tree (from tensorflow_datasets)\n",
      "  Downloading dm_tree-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting etils>=1.9.1 (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets)\n",
      "  Downloading etils-1.13.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting immutabledict (from tensorflow_datasets)\n",
      "  Downloading immutabledict-4.2.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (7.0.0)\n",
      "Collecting pyarrow (from tensorflow_datasets)\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting simple_parsing (from tensorflow_datasets)\n",
      "  Downloading simple_parsing-0.1.7-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Downloading tensorflow_metadata-1.17.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting toml (from tensorflow_datasets)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tqdm (from tensorflow_datasets)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting einops (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2024.10.0)\n",
      "Collecting importlib_resources (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: zipp in /usr/lib/python3/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.0.0)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.17.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow_datasets) (25.3.0)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple_parsing->tensorflow_datasets)\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow_datasets)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m241.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.6/620.6 MB\u001b[0m \u001b[31m156.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m243.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m225.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m184.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_datasets-4.9.9-py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m131.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading array_record-0.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading etils-1.13.0-py3-none-any.whl (170 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading fonttools-4.59.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m175.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.74.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m202.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m211.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading keras-3.11.2-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m192.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m188.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m216.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m256.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m231.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m222.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading wrapt-1.17.3-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (82 kB)\n",
      "Downloading dm_tree-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "Downloading immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
      "Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m274.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
      "Downloading tensorflow_metadata-1.17.2-py3-none-any.whl (31 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m211.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.17.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (402 kB)\n",
      "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21583 sha256=424b2b9200514d788201733476b91652753ff1eabda158f3d364ca8e177e1a29\n",
      "  Stored in directory: /root/.cache/pip/wheels/90/74/b1/9b54c896b8d9409e9268329d4d45ede8a8040abe91c8879932\n",
      "Successfully built promise\n",
      "Installing collected packages: pytz, namex, libclang, flatbuffers, wrapt, werkzeug, tzdata, tqdm, toml, threadpoolctl, termcolor, tensorboard-data-server, scipy, pyarrow, protobuf, promise, optree, opt_einsum, opencv-python, ml_dtypes, mdurl, markdown, kiwisolver, joblib, importlib_resources, immutabledict, h5py, grpcio, google_pasta, gast, fonttools, etils, einops, docstring-parser, cycler, contourpy, astunparse, absl-py, tensorboard, simple_parsing, scikit-learn, pandas, matplotlib, markdown-it-py, googleapis-common-protos, dm-tree, tensorflow-metadata, rich, keras, array_record, tensorflow, tensorflow_datasets\n",
      "Successfully installed absl-py-2.3.1 array_record-0.7.2 astunparse-1.6.3 contourpy-1.3.3 cycler-0.12.1 dm-tree-0.1.9 docstring-parser-0.17.0 einops-0.8.1 etils-1.13.0 flatbuffers-25.2.10 fonttools-4.59.1 gast-0.6.0 google_pasta-0.2.0 googleapis-common-protos-1.70.0 grpcio-1.74.0 h5py-3.14.0 immutabledict-4.2.1 importlib_resources-6.5.2 joblib-1.5.1 keras-3.11.2 kiwisolver-1.4.9 libclang-18.1.1 markdown-3.8.2 markdown-it-py-4.0.0 matplotlib-3.10.5 mdurl-0.1.2 ml_dtypes-0.5.3 namex-0.1.0 opencv-python-4.12.0.88 opt_einsum-3.4.0 optree-0.17.0 pandas-2.3.1 promise-2.3 protobuf-6.32.0 pyarrow-21.0.0 pytz-2025.2 rich-14.1.0 scikit-learn-1.7.1 scipy-1.16.1 simple_parsing-0.1.7 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 tensorflow-metadata-1.17.2 tensorflow_datasets-4.9.9 termcolor-3.1.0 threadpoolctl-3.6.0 toml-0.10.2 tqdm-4.67.1 tzdata-2025.2 werkzeug-3.1.3 wrapt-1.17.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U opencv-python tensorflow scikit-learn pandas matplotlib tensorflow_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTATION DES LIBRAIRIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 14:03:16.775500: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-15 14:03:16.836470: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-15 14:03:18.259083: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets.public_api as tfds\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def telecharger_dezip(url, chemin_sauv=\"plant_village_dataset.zip\", extract_path=\".\"):\n",
    "    print(\" Début du téléchargement\")\n",
    "    try:\n",
    "        response=requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        #Taille totale du fichier pour la barre de progression\n",
    "        total_size=int(response.headers.get('content-length',0))\n",
    "        block_size=1064\n",
    "        bar_progression = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "\n",
    "        #Téléchargement\n",
    "        with open(chemin_sauv, 'wb') as file:\n",
    "            for data in response.iter_content(block_size):\n",
    "                bar_progression.update(len(data))\n",
    "                file.write(data)\n",
    "        bar_progression.close()\n",
    "\n",
    "        if total_size != 0 and bar_progression.n != total_size:\n",
    "            print(\"ERREUR, quelque chose s'est mal passé pendant le téléchargement.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Téléchargement terminé. Fichier sauvegardé sous : {chemin_sauv}\")\n",
    "\n",
    "        # Créer le dossier d'extraction s'il n'existe pas\n",
    "        if not os.path.exists(extract_path):\n",
    "            os.makedirs(extract_path)\n",
    "\n",
    "        # Décompresser le fichier ZIP\n",
    "        print(f\"Décompression du fichier dans le dossier : {extract_path}\")\n",
    "        with zipfile.ZipFile(chemin_sauv, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "\n",
    "        print(\"Décompression terminée.\")\n",
    "\n",
    "        # Optionnel : Supprimer le fichier .zip après extraction pour économiser de l'espace\n",
    "        print(f\"Suppression du fichier {chemin_sauv}...\")\n",
    "        os.remove(chemin_sauv)\n",
    "        print(\"Opération terminée avec succès !\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Une erreur de réseau est survenue: {e}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"Erreur: Le fichier téléchargé n'est pas un fichier ZIP valide.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur inattendue est survenue: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://data.mendeley.com/datasets/tywbtsjrjv/1/files/b4e3a32f-c0bd-4060-81e9-6144231f2520/file_downloaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_folder = \"plant_village_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Début du téléchargement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 949M/949M [00:49<00:00, 19.0MiB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement terminé. Fichier sauvegardé sous : PlantVillage.zip\n",
      "Décompression du fichier dans le dossier : plant_village_dataset\n",
      "Décompression terminée.\n",
      "Suppression du fichier PlantVillage.zip...\n",
      "Opération terminée avec succès !\n"
     ]
    }
   ],
   "source": [
    "telecharger_dezip(URL, \"PlantVillage.zip\", extract_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/workspace/plant_village_dataset/Plant_leave_diseases_dataset_with_augmentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "TEMPERATURE = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61486 images belonging to 39 classes.\n"
     ]
    }
   ],
   "source": [
    "data=data_gen.flow_from_directory(\n",
    "    path,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TS avec \n",
    "\n",
    "**Teacher:resnet50**\n",
    "\n",
    "**Student:efficientnet_b0**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total des images trouvées : 61486\n"
     ]
    }
   ],
   "source": [
    "# --------- 1. Préparer les données ---------\n",
    "filepaths = []\n",
    "labels = []\n",
    "folds = os.listdir(path)\n",
    "for fold in folds:\n",
    "    f_path = os.path.join(path, fold)\n",
    "    if not os.path.isdir(f_path):\n",
    "        continue\n",
    "    for file in os.listdir(f_path):\n",
    "        filepaths.append(os.path.join(f_path, file))\n",
    "        labels.append(fold)\n",
    "\n",
    "df = pd.DataFrame({'filepaths': filepaths, 'labels': labels})\n",
    "print(f\"Total des images trouvées : {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 80/20 avec stratification\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=df['labels']\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df['labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping des classes en indices\n",
    "class_names = sorted(df['labels'].unique())\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- 2. Dataset personnalisé ---------\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, class_to_idx, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.loc[idx, 'filepaths']\n",
    "        label_name = self.df.loc[idx, 'labels']\n",
    "        label = self.class_to_idx[label_name]\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- 3. Data augmentation et loaders ---------\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]) # EfficientNet normalization\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomImageDataset(train_df, class_to_idx, transform=train_transforms)\n",
    "val_dataset = CustomImageDataset(val_df, class_to_idx, transform=val_transforms)\n",
    "test_dataset = CustomImageDataset(test_df, class_to_idx, transform=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Name: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU Name: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle Teacher (Professeur) - ResNet50\n",
    "teacher_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs_teacher = teacher_model.fc.in_features\n",
    "teacher_model.fc = nn.Linear(num_ftrs_teacher, num_classes)\n",
    "teacher_model = teacher_model.to(device)\n",
    "\n",
    "# Modèle Student (Étudiant) \n",
    "student_model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "num_ftrs_student = student_model.classifier[1].in_features\n",
    "student_model.classifier[1] = nn.Linear(num_ftrs_student, num_classes)\n",
    "student_model = student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_hard = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Distillation Loss ----------\n",
    "def distillation_loss(student_outputs, teacher_outputs, temperature, device):\n",
    "    teacher_outputs = teacher_outputs.detach()\n",
    "    soft_teacher_outputs = nn.functional.softmax(teacher_outputs / temperature, dim=1)\n",
    "    soft_student_outputs = nn.functional.log_softmax(student_outputs / temperature, dim=1)\n",
    "    dist_loss = nn.functional.kl_div(soft_student_outputs, soft_teacher_outputs, reduction='batchmean')\n",
    "    return dist_loss * (temperature * temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_student = optim.Adam(student_model.parameters(), lr=0.001)\n",
    "scheduler_student = optim.lr_scheduler.StepLR(optimizer_student, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_teacher_student(teacher_model, student_model, criterion_hard, optimizer_student, scheduler_student, num_epochs, train_loader, val_loader, device):\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_path = '/workspace/models/best_student_efficientnetb0_model.pth'\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        student_model.train()\n",
    "        teacher_model.eval()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer_student.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(inputs)\n",
    "            \n",
    "            student_outputs = student_model(inputs)\n",
    "            \n",
    "            hard_loss = criterion_hard(student_outputs, labels)\n",
    "            soft_loss = distillation_loss(student_outputs, teacher_outputs, TEMPERATURE, device)\n",
    "            \n",
    "            loss = hard_loss * 0.5 + soft_loss * 0.5\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer_student.step()\n",
    "            \n",
    "            _, preds = torch.max(student_outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        scheduler_student.step()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        student_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = student_model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion_hard(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} | '\n",
    "              f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "        \n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            torch.save(student_model.state_dict(), best_model_path)\n",
    "            print(f'Meilleur modèle étudiant sauvegardé avec une précision de validation de {best_val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/20: 100%|██████████| 1384/1384 [01:39<00:00, 13.92it/s]\n",
      "Validation Epoch 1/20: 100%|██████████| 346/346 [00:19<00:00, 17.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 0.5906 Acc: 0.9416 | Val Loss: 0.5068 Acc: 0.9757\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/20: 100%|██████████| 1384/1384 [01:21<00:00, 17.04it/s]\n",
      "Validation Epoch 2/20: 100%|██████████| 346/346 [00:17<00:00, 19.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Train Loss: 0.5636 Acc: 0.9523 | Val Loss: 0.4749 Acc: 0.9816\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/20: 100%|██████████| 1384/1384 [01:20<00:00, 17.18it/s]\n",
      "Validation Epoch 3/20: 100%|██████████| 346/346 [00:17<00:00, 19.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Train Loss: 0.5560 Acc: 0.9536 | Val Loss: 0.4807 Acc: 0.9724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/20: 100%|██████████| 1384/1384 [01:21<00:00, 17.05it/s]\n",
      "Validation Epoch 4/20: 100%|██████████| 346/346 [00:15<00:00, 22.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Train Loss: 0.5434 Acc: 0.9594 | Val Loss: 0.4290 Acc: 0.9909\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/20: 100%|██████████| 1384/1384 [01:16<00:00, 17.98it/s]\n",
      "Validation Epoch 5/20: 100%|██████████| 346/346 [00:16<00:00, 20.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Train Loss: 0.5386 Acc: 0.9605 | Val Loss: 0.4583 Acc: 0.9870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/20: 100%|██████████| 1384/1384 [01:18<00:00, 17.68it/s]\n",
      "Validation Epoch 6/20: 100%|██████████| 346/346 [00:15<00:00, 21.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Train Loss: 0.5302 Acc: 0.9639 | Val Loss: 0.4578 Acc: 0.9859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/20: 100%|██████████| 1384/1384 [01:17<00:00, 17.84it/s]\n",
      "Validation Epoch 7/20: 100%|██████████| 346/346 [00:17<00:00, 20.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Train Loss: 0.5001 Acc: 0.9798 | Val Loss: 0.4066 Acc: 0.9951\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/20: 100%|██████████| 1384/1384 [01:16<00:00, 18.00it/s]\n",
      "Validation Epoch 8/20: 100%|██████████| 346/346 [00:16<00:00, 21.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Train Loss: 0.4911 Acc: 0.9838 | Val Loss: 0.4012 Acc: 0.9962\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/20: 100%|██████████| 1384/1384 [01:17<00:00, 17.86it/s]\n",
      "Validation Epoch 9/20: 100%|██████████| 346/346 [00:16<00:00, 21.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Train Loss: 0.4878 Acc: 0.9858 | Val Loss: 0.4181 Acc: 0.9951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/20: 100%|██████████| 1384/1384 [01:19<00:00, 17.41it/s]\n",
      "Validation Epoch 10/20: 100%|██████████| 346/346 [00:15<00:00, 22.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Train Loss: 0.4859 Acc: 0.9864 | Val Loss: 0.4057 Acc: 0.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/20: 100%|██████████| 1384/1384 [01:17<00:00, 17.82it/s]\n",
      "Validation Epoch 11/20: 100%|██████████| 346/346 [00:15<00:00, 22.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Train Loss: 0.4834 Acc: 0.9876 | Val Loss: 0.4178 Acc: 0.9964\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/20: 100%|██████████| 1384/1384 [01:18<00:00, 17.69it/s]\n",
      "Validation Epoch 12/20: 100%|██████████| 346/346 [00:16<00:00, 21.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Train Loss: 0.4821 Acc: 0.9885 | Val Loss: 0.4092 Acc: 0.9968\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/20: 100%|██████████| 1384/1384 [01:23<00:00, 16.59it/s]\n",
      "Validation Epoch 13/20: 100%|██████████| 346/346 [00:16<00:00, 21.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Train Loss: 0.4807 Acc: 0.9892 | Val Loss: 0.4076 Acc: 0.9970\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/20: 100%|██████████| 1384/1384 [01:17<00:00, 17.91it/s]\n",
      "Validation Epoch 14/20: 100%|██████████| 346/346 [00:16<00:00, 20.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Train Loss: 0.4811 Acc: 0.9886 | Val Loss: 0.4013 Acc: 0.9972\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/20: 100%|██████████| 1384/1384 [01:20<00:00, 17.11it/s]\n",
      "Validation Epoch 15/20: 100%|██████████| 346/346 [00:17<00:00, 19.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Train Loss: 0.4792 Acc: 0.9898 | Val Loss: 0.4090 Acc: 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/20: 100%|██████████| 1384/1384 [01:20<00:00, 17.12it/s]\n",
      "Validation Epoch 16/20: 100%|██████████| 346/346 [00:17<00:00, 19.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Train Loss: 0.4790 Acc: 0.9898 | Val Loss: 0.4107 Acc: 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/20: 100%|██████████| 1384/1384 [01:18<00:00, 17.59it/s]\n",
      "Validation Epoch 17/20: 100%|██████████| 346/346 [00:16<00:00, 21.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Train Loss: 0.4791 Acc: 0.9897 | Val Loss: 0.4123 Acc: 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/20: 100%|██████████| 1384/1384 [01:17<00:00, 17.86it/s]\n",
      "Validation Epoch 18/20: 100%|██████████| 346/346 [00:17<00:00, 20.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Train Loss: 0.4789 Acc: 0.9899 | Val Loss: 0.4080 Acc: 0.9973\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/20: 100%|██████████| 1384/1384 [01:23<00:00, 16.54it/s]\n",
      "Validation Epoch 19/20: 100%|██████████| 346/346 [00:16<00:00, 20.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Train Loss: 0.4783 Acc: 0.9904 | Val Loss: 0.4036 Acc: 0.9975\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/20: 100%|██████████| 1384/1384 [01:21<00:00, 17.05it/s]\n",
      "Validation Epoch 20/20: 100%|██████████| 346/346 [00:16<00:00, 20.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Train Loss: 0.4776 Acc: 0.9908 | Val Loss: 0.3977 Acc: 0.9974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 20\n",
    "train_teacher_student(teacher_model, student_model, criterion_hard, optimizer_student, scheduler_student, NUM_EPOCHS, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, psutil, torch\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formation du modèle étudiant terminée. Chargement du meilleur modèle pour l'évaluation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (8): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=True)\n",
       "    (1): Linear(in_features=1280, out_features=39, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nFormation du modèle étudiant terminée. Chargement du meilleur modèle pour l'évaluation.\")\n",
    "student_model.load_state_dict(torch.load('/workspace/models/best_student_efficientnetb0_model.pth'))\n",
    "student_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1] Time: 0.04s | CPU: 15.7% | RAM: 45.28GB | GPU: 203.49MB\n",
      "[Batch 2] Time: 0.01s | CPU: 98.7% | RAM: 45.31GB | GPU: 203.49MB\n",
      "[Batch 3] Time: 0.01s | CPU: 92.3% | RAM: 45.32GB | GPU: 203.49MB\n",
      "[Batch 4] Time: 0.01s | CPU: 98.1% | RAM: 45.32GB | GPU: 203.49MB\n",
      "[Batch 5] Time: 0.01s | CPU: 87.6% | RAM: 45.31GB | GPU: 203.49MB\n",
      "[Batch 6] Time: 0.01s | CPU: 62.8% | RAM: 45.36GB | GPU: 203.49MB\n",
      "[Batch 7] Time: 0.01s | CPU: 58.9% | RAM: 45.30GB | GPU: 203.49MB\n",
      "[Batch 8] Time: 0.01s | CPU: 51.3% | RAM: 45.30GB | GPU: 203.49MB\n",
      "[Batch 9] Time: 0.01s | CPU: 49.8% | RAM: 45.34GB | GPU: 203.49MB\n",
      "[Batch 10] Time: 0.01s | CPU: 59.8% | RAM: 45.34GB | GPU: 203.49MB\n",
      "[Batch 11] Time: 0.01s | CPU: 60.6% | RAM: 45.35GB | GPU: 203.49MB\n",
      "[Batch 12] Time: 0.01s | CPU: 59.7% | RAM: 45.35GB | GPU: 203.49MB\n",
      "[Batch 13] Time: 0.01s | CPU: 94.6% | RAM: 45.36GB | GPU: 203.49MB\n",
      "[Batch 14] Time: 0.01s | CPU: 98.2% | RAM: 45.35GB | GPU: 203.49MB\n",
      "[Batch 15] Time: 0.01s | CPU: 98.8% | RAM: 45.36GB | GPU: 203.49MB\n",
      "[Batch 16] Time: 0.01s | CPU: 99.6% | RAM: 45.37GB | GPU: 203.49MB\n",
      "[Batch 17] Time: 0.02s | CPU: 99.1% | RAM: 45.36GB | GPU: 203.49MB\n",
      "[Batch 18] Time: 0.01s | CPU: 98.8% | RAM: 45.38GB | GPU: 203.49MB\n",
      "[Batch 19] Time: 0.01s | CPU: 99.5% | RAM: 45.39GB | GPU: 203.49MB\n",
      "[Batch 20] Time: 0.01s | CPU: 99.2% | RAM: 45.39GB | GPU: 203.49MB\n",
      "[Batch 21] Time: 0.01s | CPU: 99.3% | RAM: 45.41GB | GPU: 203.49MB\n",
      "[Batch 22] Time: 0.01s | CPU: 97.4% | RAM: 45.46GB | GPU: 203.49MB\n",
      "[Batch 23] Time: 0.01s | CPU: 98.9% | RAM: 45.46GB | GPU: 203.49MB\n",
      "[Batch 24] Time: 0.01s | CPU: 99.1% | RAM: 45.41GB | GPU: 203.49MB\n",
      "[Batch 25] Time: 0.01s | CPU: 85.4% | RAM: 45.45GB | GPU: 203.49MB\n",
      "[Batch 26] Time: 0.01s | CPU: 94.8% | RAM: 45.52GB | GPU: 203.49MB\n",
      "[Batch 27] Time: 0.01s | CPU: 99.0% | RAM: 45.51GB | GPU: 203.49MB\n",
      "[Batch 28] Time: 0.01s | CPU: 94.6% | RAM: 45.53GB | GPU: 203.49MB\n",
      "[Batch 29] Time: 0.01s | CPU: 29.7% | RAM: 45.59GB | GPU: 203.49MB\n",
      "[Batch 30] Time: 0.01s | CPU: 7.8% | RAM: 45.57GB | GPU: 203.49MB\n",
      "[Batch 31] Time: 0.01s | CPU: 8.4% | RAM: 45.55GB | GPU: 203.49MB\n",
      "[Batch 32] Time: 0.01s | CPU: 8.7% | RAM: 45.55GB | GPU: 203.49MB\n",
      "[Batch 33] Time: 0.01s | CPU: 39.4% | RAM: 45.55GB | GPU: 203.49MB\n",
      "[Batch 34] Time: 0.01s | CPU: 60.2% | RAM: 45.57GB | GPU: 203.49MB\n",
      "[Batch 35] Time: 0.01s | CPU: 61.3% | RAM: 45.59GB | GPU: 203.49MB\n",
      "[Batch 36] Time: 0.02s | CPU: 97.6% | RAM: 45.60GB | GPU: 203.49MB\n",
      "[Batch 37] Time: 0.01s | CPU: 100.0% | RAM: 45.60GB | GPU: 203.49MB\n",
      "[Batch 38] Time: 0.01s | CPU: 93.1% | RAM: 45.61GB | GPU: 203.49MB\n",
      "[Batch 39] Time: 0.01s | CPU: 98.5% | RAM: 45.60GB | GPU: 203.49MB\n",
      "[Batch 40] Time: 0.01s | CPU: 98.7% | RAM: 45.59GB | GPU: 203.49MB\n",
      "[Batch 41] Time: 0.02s | CPU: 99.3% | RAM: 45.59GB | GPU: 203.49MB\n",
      "[Batch 42] Time: 0.01s | CPU: 98.3% | RAM: 45.58GB | GPU: 203.49MB\n",
      "[Batch 43] Time: 0.01s | CPU: 97.5% | RAM: 45.59GB | GPU: 203.49MB\n",
      "[Batch 44] Time: 0.01s | CPU: 97.8% | RAM: 45.62GB | GPU: 203.49MB\n",
      "[Batch 45] Time: 0.01s | CPU: 99.3% | RAM: 45.62GB | GPU: 203.49MB\n",
      "[Batch 46] Time: 0.01s | CPU: 98.7% | RAM: 45.64GB | GPU: 203.49MB\n",
      "[Batch 47] Time: 0.01s | CPU: 99.5% | RAM: 45.65GB | GPU: 203.49MB\n",
      "[Batch 48] Time: 0.01s | CPU: 94.4% | RAM: 45.65GB | GPU: 203.49MB\n",
      "[Batch 49] Time: 0.01s | CPU: 99.2% | RAM: 45.65GB | GPU: 203.49MB\n",
      "[Batch 50] Time: 0.01s | CPU: 99.5% | RAM: 45.68GB | GPU: 203.49MB\n",
      "[Batch 51] Time: 0.01s | CPU: 99.5% | RAM: 45.67GB | GPU: 203.49MB\n",
      "[Batch 52] Time: 0.01s | CPU: 65.7% | RAM: 45.75GB | GPU: 203.49MB\n",
      "[Batch 53] Time: 0.01s | CPU: 14.1% | RAM: 45.76GB | GPU: 203.49MB\n",
      "[Batch 54] Time: 0.01s | CPU: 59.3% | RAM: 45.65GB | GPU: 203.49MB\n",
      "[Batch 55] Time: 0.01s | CPU: 61.3% | RAM: 45.65GB | GPU: 203.49MB\n",
      "[Batch 56] Time: 0.01s | CPU: 62.2% | RAM: 45.69GB | GPU: 203.49MB\n",
      "[Batch 57] Time: 0.01s | CPU: 58.8% | RAM: 45.69GB | GPU: 203.49MB\n",
      "[Batch 58] Time: 0.01s | CPU: 57.1% | RAM: 45.69GB | GPU: 203.49MB\n",
      "[Batch 59] Time: 0.01s | CPU: 56.5% | RAM: 45.69GB | GPU: 203.49MB\n",
      "[Batch 60] Time: 0.01s | CPU: 6.9% | RAM: 45.68GB | GPU: 203.49MB\n",
      "[Batch 61] Time: 0.01s | CPU: 5.9% | RAM: 45.68GB | GPU: 203.49MB\n",
      "[Batch 62] Time: 0.01s | CPU: 6.2% | RAM: 45.68GB | GPU: 203.49MB\n",
      "[Batch 63] Time: 0.01s | CPU: 6.7% | RAM: 45.69GB | GPU: 203.49MB\n",
      "[Batch 64] Time: 0.02s | CPU: 7.2% | RAM: 45.70GB | GPU: 203.49MB\n",
      "[Batch 65] Time: 0.01s | CPU: 17.2% | RAM: 45.70GB | GPU: 203.49MB\n",
      "[Batch 66] Time: 0.02s | CPU: 29.5% | RAM: 45.69GB | GPU: 203.49MB\n",
      "[Batch 67] Time: 0.01s | CPU: 7.1% | RAM: 45.69GB | GPU: 203.49MB\n",
      "[Batch 68] Time: 0.02s | CPU: 6.5% | RAM: 45.69GB | GPU: 203.49MB\n",
      "[Batch 69] Time: 0.01s | CPU: 7.5% | RAM: 45.68GB | GPU: 203.49MB\n",
      "[Batch 70] Time: 0.01s | CPU: 6.3% | RAM: 45.68GB | GPU: 203.49MB\n",
      "[Batch 71] Time: 0.01s | CPU: 21.8% | RAM: 45.69GB | GPU: 203.49MB\n",
      "[Batch 72] Time: 0.02s | CPU: 30.6% | RAM: 45.70GB | GPU: 203.49MB\n",
      "[Batch 73] Time: 0.01s | CPU: 6.2% | RAM: 45.70GB | GPU: 203.49MB\n",
      "[Batch 74] Time: 0.02s | CPU: 6.4% | RAM: 45.70GB | GPU: 203.49MB\n",
      "[Batch 75] Time: 0.01s | CPU: 6.9% | RAM: 45.69GB | GPU: 203.49MB\n",
      "[Batch 76] Time: 0.01s | CPU: 6.6% | RAM: 45.70GB | GPU: 203.49MB\n",
      "[Batch 77] Time: 0.01s | CPU: 7.1% | RAM: 45.70GB | GPU: 203.49MB\n",
      "[Batch 78] Time: 0.02s | CPU: 23.3% | RAM: 45.66GB | GPU: 203.49MB\n",
      "[Batch 79] Time: 0.01s | CPU: 6.9% | RAM: 45.66GB | GPU: 203.49MB\n",
      "[Batch 80] Time: 0.01s | CPU: 6.4% | RAM: 45.67GB | GPU: 203.49MB\n",
      "[Batch 81] Time: 0.01s | CPU: 7.0% | RAM: 45.66GB | GPU: 203.49MB\n",
      "[Batch 82] Time: 0.01s | CPU: 9.3% | RAM: 45.64GB | GPU: 203.49MB\n",
      "[Batch 83] Time: 0.02s | CPU: 18.4% | RAM: 45.63GB | GPU: 203.49MB\n",
      "[Batch 84] Time: 0.01s | CPU: 67.9% | RAM: 45.63GB | GPU: 203.49MB\n",
      "[Batch 85] Time: 0.01s | CPU: 40.0% | RAM: 45.63GB | GPU: 203.49MB\n",
      "[Batch 86] Time: 0.01s | CPU: 8.3% | RAM: 45.63GB | GPU: 203.49MB\n",
      "[Batch 87] Time: 0.01s | CPU: 7.6% | RAM: 45.63GB | GPU: 203.49MB\n",
      "[Batch 88] Time: 0.02s | CPU: 9.2% | RAM: 45.63GB | GPU: 203.49MB\n",
      "[Batch 89] Time: 0.01s | CPU: 8.8% | RAM: 45.64GB | GPU: 203.49MB\n",
      "[Batch 90] Time: 0.02s | CPU: 80.0% | RAM: 45.67GB | GPU: 203.49MB\n",
      "[Batch 91] Time: 0.01s | CPU: 100.0% | RAM: 45.67GB | GPU: 203.49MB\n",
      "[Batch 92] Time: 0.01s | CPU: 100.0% | RAM: 45.67GB | GPU: 203.49MB\n",
      "[Batch 93] Time: 0.01s | CPU: 99.6% | RAM: 45.67GB | GPU: 203.49MB\n",
      "[Batch 94] Time: 0.01s | CPU: 91.8% | RAM: 45.66GB | GPU: 203.49MB\n",
      "[Batch 95] Time: 0.01s | CPU: 64.6% | RAM: 45.65GB | GPU: 203.49MB\n",
      "[Batch 96] Time: 0.01s | CPU: 64.7% | RAM: 45.67GB | GPU: 203.49MB\n",
      "[Batch 97] Time: 0.01s | CPU: 67.7% | RAM: 45.68GB | GPU: 203.49MB\n",
      "[Batch 98] Time: 0.01s | CPU: 61.2% | RAM: 45.68GB | GPU: 203.49MB\n",
      "[Batch 99] Time: 0.01s | CPU: 40.7% | RAM: 45.67GB | GPU: 203.49MB\n",
      "[Batch 100] Time: 0.01s | CPU: 64.8% | RAM: 45.67GB | GPU: 203.49MB\n",
      "[Batch 101] Time: 0.01s | CPU: 66.7% | RAM: 45.66GB | GPU: 203.49MB\n",
      "[Batch 102] Time: 0.01s | CPU: 66.8% | RAM: 45.64GB | GPU: 203.49MB\n",
      "[Batch 103] Time: 0.01s | CPU: 99.1% | RAM: 45.64GB | GPU: 203.49MB\n",
      "[Batch 104] Time: 0.01s | CPU: 99.2% | RAM: 45.64GB | GPU: 203.49MB\n",
      "[Batch 105] Time: 0.01s | CPU: 99.2% | RAM: 45.65GB | GPU: 203.49MB\n",
      "[Batch 106] Time: 0.01s | CPU: 80.3% | RAM: 45.65GB | GPU: 203.49MB\n",
      "[Batch 107] Time: 0.01s | CPU: 59.8% | RAM: 45.66GB | GPU: 203.49MB\n",
      "[Batch 108] Time: 0.01s | CPU: 62.0% | RAM: 45.68GB | GPU: 203.49MB\n",
      "[Batch 109] Time: 0.01s | CPU: 59.5% | RAM: 45.69GB | GPU: 203.49MB\n",
      "[Batch 110] Time: 0.01s | CPU: 62.7% | RAM: 45.72GB | GPU: 203.49MB\n",
      "[Batch 111] Time: 0.01s | CPU: 61.9% | RAM: 45.71GB | GPU: 203.49MB\n",
      "[Batch 112] Time: 0.01s | CPU: 64.2% | RAM: 45.73GB | GPU: 203.49MB\n",
      "[Batch 113] Time: 0.01s | CPU: 64.5% | RAM: 45.73GB | GPU: 203.49MB\n",
      "[Batch 114] Time: 0.01s | CPU: 98.4% | RAM: 45.75GB | GPU: 203.49MB\n",
      "[Batch 115] Time: 0.01s | CPU: 99.1% | RAM: 45.75GB | GPU: 203.49MB\n",
      "[Batch 116] Time: 0.01s | CPU: 99.5% | RAM: 45.74GB | GPU: 203.49MB\n",
      "[Batch 117] Time: 0.01s | CPU: 100.0% | RAM: 45.74GB | GPU: 203.49MB\n",
      "[Batch 118] Time: 0.01s | CPU: 83.1% | RAM: 45.75GB | GPU: 203.49MB\n",
      "[Batch 119] Time: 0.01s | CPU: 64.5% | RAM: 45.75GB | GPU: 203.49MB\n",
      "[Batch 120] Time: 0.01s | CPU: 64.7% | RAM: 45.76GB | GPU: 203.49MB\n",
      "[Batch 121] Time: 0.01s | CPU: 64.9% | RAM: 45.76GB | GPU: 203.49MB\n",
      "[Batch 122] Time: 0.01s | CPU: 61.3% | RAM: 45.75GB | GPU: 203.49MB\n",
      "[Batch 123] Time: 0.01s | CPU: 59.7% | RAM: 45.76GB | GPU: 203.49MB\n",
      "[Batch 124] Time: 0.01s | CPU: 83.9% | RAM: 45.75GB | GPU: 203.49MB\n",
      "[Batch 125] Time: 0.01s | CPU: 68.1% | RAM: 45.76GB | GPU: 203.49MB\n",
      "[Batch 126] Time: 0.01s | CPU: 69.0% | RAM: 45.78GB | GPU: 203.49MB\n",
      "[Batch 127] Time: 0.01s | CPU: 99.0% | RAM: 45.78GB | GPU: 203.49MB\n",
      "[Batch 128] Time: 0.01s | CPU: 80.5% | RAM: 45.78GB | GPU: 203.49MB\n",
      "[Batch 129] Time: 0.01s | CPU: 63.4% | RAM: 45.78GB | GPU: 203.49MB\n",
      "[Batch 130] Time: 0.01s | CPU: 84.6% | RAM: 45.80GB | GPU: 203.49MB\n",
      "[Batch 131] Time: 0.01s | CPU: 65.3% | RAM: 45.80GB | GPU: 203.49MB\n",
      "[Batch 132] Time: 0.01s | CPU: 68.1% | RAM: 45.81GB | GPU: 203.49MB\n",
      "[Batch 133] Time: 0.01s | CPU: 66.5% | RAM: 45.81GB | GPU: 203.49MB\n",
      "[Batch 134] Time: 0.01s | CPU: 31.3% | RAM: 45.80GB | GPU: 203.49MB\n",
      "[Batch 135] Time: 0.01s | CPU: 7.3% | RAM: 45.79GB | GPU: 203.49MB\n",
      "[Batch 136] Time: 0.01s | CPU: 7.5% | RAM: 45.81GB | GPU: 203.49MB\n",
      "[Batch 137] Time: 0.01s | CPU: 7.2% | RAM: 45.81GB | GPU: 203.49MB\n",
      "[Batch 138] Time: 0.02s | CPU: 7.5% | RAM: 45.82GB | GPU: 203.49MB\n",
      "[Batch 139] Time: 0.02s | CPU: 7.4% | RAM: 45.84GB | GPU: 203.49MB\n",
      "[Batch 140] Time: 0.02s | CPU: 7.3% | RAM: 45.84GB | GPU: 203.49MB\n",
      "[Batch 141] Time: 0.01s | CPU: 7.0% | RAM: 45.84GB | GPU: 203.49MB\n",
      "[Batch 142] Time: 0.02s | CPU: 7.5% | RAM: 45.83GB | GPU: 203.49MB\n",
      "[Batch 143] Time: 0.02s | CPU: 8.4% | RAM: 45.82GB | GPU: 203.49MB\n",
      "[Batch 144] Time: 0.02s | CPU: 12.1% | RAM: 45.81GB | GPU: 203.49MB\n",
      "[Batch 145] Time: 0.01s | CPU: 7.7% | RAM: 45.81GB | GPU: 203.49MB\n",
      "[Batch 146] Time: 0.02s | CPU: 6.4% | RAM: 45.84GB | GPU: 203.49MB\n",
      "[Batch 147] Time: 0.02s | CPU: 6.9% | RAM: 45.84GB | GPU: 203.49MB\n",
      "[Batch 148] Time: 0.02s | CPU: 6.3% | RAM: 45.86GB | GPU: 203.49MB\n",
      "[Batch 149] Time: 0.01s | CPU: 6.4% | RAM: 45.85GB | GPU: 203.49MB\n",
      "[Batch 150] Time: 0.02s | CPU: 6.8% | RAM: 45.96GB | GPU: 203.49MB\n",
      "[Batch 151] Time: 0.02s | CPU: 6.6% | RAM: 45.96GB | GPU: 203.49MB\n",
      "[Batch 152] Time: 0.02s | CPU: 6.0% | RAM: 45.94GB | GPU: 203.49MB\n",
      "[Batch 153] Time: 0.01s | CPU: 6.6% | RAM: 45.93GB | GPU: 203.49MB\n",
      "[Batch 154] Time: 0.02s | CPU: 5.8% | RAM: 45.92GB | GPU: 203.49MB\n",
      "[Batch 155] Time: 0.02s | CPU: 6.8% | RAM: 45.92GB | GPU: 203.49MB\n",
      "[Batch 156] Time: 0.02s | CPU: 6.5% | RAM: 45.90GB | GPU: 203.49MB\n",
      "[Batch 157] Time: 0.01s | CPU: 5.8% | RAM: 45.90GB | GPU: 203.49MB\n",
      "[Batch 158] Time: 0.02s | CPU: 6.1% | RAM: 45.93GB | GPU: 203.49MB\n",
      "[Batch 159] Time: 0.02s | CPU: 6.2% | RAM: 45.93GB | GPU: 203.49MB\n",
      "[Batch 160] Time: 0.02s | CPU: 7.1% | RAM: 45.93GB | GPU: 203.49MB\n",
      "[Batch 161] Time: 0.01s | CPU: 6.0% | RAM: 45.93GB | GPU: 203.49MB\n",
      "[Batch 162] Time: 0.02s | CPU: 6.8% | RAM: 45.93GB | GPU: 203.49MB\n",
      "[Batch 163] Time: 0.02s | CPU: 5.9% | RAM: 45.92GB | GPU: 203.49MB\n",
      "[Batch 164] Time: 0.02s | CPU: 6.7% | RAM: 45.94GB | GPU: 203.49MB\n",
      "[Batch 165] Time: 0.01s | CPU: 6.5% | RAM: 45.94GB | GPU: 203.49MB\n",
      "[Batch 166] Time: 0.02s | CPU: 6.6% | RAM: 46.02GB | GPU: 203.49MB\n",
      "[Batch 167] Time: 0.02s | CPU: 6.6% | RAM: 46.04GB | GPU: 203.49MB\n",
      "[Batch 168] Time: 0.02s | CPU: 6.2% | RAM: 46.04GB | GPU: 203.49MB\n",
      "[Batch 169] Time: 0.01s | CPU: 6.9% | RAM: 46.04GB | GPU: 203.49MB\n",
      "[Batch 170] Time: 0.02s | CPU: 6.1% | RAM: 46.08GB | GPU: 203.49MB\n",
      "[Batch 171] Time: 0.02s | CPU: 6.5% | RAM: 46.08GB | GPU: 203.49MB\n",
      "[Batch 172] Time: 0.02s | CPU: 7.0% | RAM: 46.08GB | GPU: 203.49MB\n",
      "[Batch 173] Time: 0.01s | CPU: 6.8% | RAM: 46.08GB | GPU: 203.49MB\n",
      "[Batch 174] Time: 0.02s | CPU: 6.6% | RAM: 46.07GB | GPU: 203.49MB\n",
      "[Batch 175] Time: 0.02s | CPU: 6.4% | RAM: 46.06GB | GPU: 203.49MB\n",
      "[Batch 176] Time: 0.02s | CPU: 6.7% | RAM: 46.05GB | GPU: 203.49MB\n",
      "[Batch 177] Time: 0.01s | CPU: 6.2% | RAM: 46.05GB | GPU: 203.49MB\n",
      "[Batch 178] Time: 0.02s | CPU: 6.2% | RAM: 46.06GB | GPU: 203.49MB\n",
      "[Batch 179] Time: 0.02s | CPU: 6.5% | RAM: 46.06GB | GPU: 203.49MB\n",
      "[Batch 180] Time: 0.02s | CPU: 6.7% | RAM: 46.06GB | GPU: 203.49MB\n",
      "[Batch 181] Time: 0.01s | CPU: 7.1% | RAM: 46.07GB | GPU: 203.49MB\n",
      "[Batch 182] Time: 0.02s | CPU: 8.7% | RAM: 46.34GB | GPU: 203.49MB\n",
      "[Batch 183] Time: 0.02s | CPU: 10.1% | RAM: 46.35GB | GPU: 203.49MB\n",
      "[Batch 184] Time: 0.02s | CPU: 8.5% | RAM: 46.38GB | GPU: 203.49MB\n",
      "[Batch 185] Time: 0.01s | CPU: 9.1% | RAM: 46.40GB | GPU: 203.49MB\n",
      "[Batch 186] Time: 0.02s | CPU: 10.0% | RAM: 46.47GB | GPU: 203.49MB\n",
      "[Batch 187] Time: 0.02s | CPU: 11.9% | RAM: 46.49GB | GPU: 203.49MB\n",
      "[Batch 188] Time: 0.02s | CPU: 16.2% | RAM: 46.54GB | GPU: 203.49MB\n",
      "[Batch 189] Time: 0.01s | CPU: 19.6% | RAM: 46.57GB | GPU: 203.49MB\n",
      "[Batch 190] Time: 0.01s | CPU: 20.2% | RAM: 46.60GB | GPU: 203.49MB\n",
      "[Batch 191] Time: 0.01s | CPU: 22.4% | RAM: 46.60GB | GPU: 203.49MB\n",
      "[Batch 192] Time: 0.01s | CPU: 20.8% | RAM: 46.60GB | GPU: 203.49MB\n",
      "[Batch 193] Time: 0.04s | CPU: 20.8% | RAM: 46.61GB | GPU: 187.98MB\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "        batch_start = time.time()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = student_model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # --- Profiling ---\n",
    "        cpu_usage = psutil.cpu_percent(interval=None)\n",
    "        ram = psutil.virtual_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "        else:\n",
    "            gpu_mem = 0.0\n",
    "        print(f\"[Batch {i+1}] Time: {time.time()-batch_start:.2f}s | CPU: {cpu_usage:.1f}% | RAM: {ram.used/1024**3:.2f}GB | GPU: {gpu_mem:.2f}MB\")\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temps Test Total: 10.74 sec\n",
      "Throughput: 572.79 images/sec\n"
     ]
    }
   ],
   "source": [
    "total_time = end_time - start_time\n",
    "print(f\"\\nTemps Test Total: {total_time:.2f} sec\")\n",
    "print(f\"Throughput: {len(test_dataset) / total_time:.2f} images/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Set Evaluation (Student) ===\n",
      "                                               precision    recall  f1-score   support\n",
      "\n",
      "                           Apple___Apple_scab       1.00      1.00      1.00       100\n",
      "                            Apple___Black_rot       1.00      1.00      1.00       100\n",
      "                     Apple___Cedar_apple_rust       1.00      1.00      1.00       100\n",
      "                              Apple___healthy       1.00      0.99      1.00       164\n",
      "                    Background_without_leaves       0.97      1.00      0.99       114\n",
      "                          Blueberry___healthy       1.00      1.00      1.00       150\n",
      "                      Cherry___Powdery_mildew       1.00      0.99      1.00       105\n",
      "                             Cherry___healthy       1.00      0.99      0.99       100\n",
      "   Corn___Cercospora_leaf_spot Gray_leaf_spot       0.98      0.99      0.99       100\n",
      "                           Corn___Common_rust       0.99      1.00      1.00       119\n",
      "                  Corn___Northern_Leaf_Blight       0.99      0.97      0.98       100\n",
      "                               Corn___healthy       1.00      1.00      1.00       116\n",
      "                            Grape___Black_rot       1.00      0.99      1.00       118\n",
      "                 Grape___Esca_(Black_Measles)       0.99      1.00      1.00       138\n",
      "   Grape___Leaf_blight_(Isariopsis_Leaf_Spot)       1.00      1.00      1.00       108\n",
      "                              Grape___healthy       1.00      1.00      1.00       100\n",
      "     Orange___Haunglongbing_(Citrus_greening)       1.00      1.00      1.00       551\n",
      "                       Peach___Bacterial_spot       1.00      1.00      1.00       230\n",
      "                              Peach___healthy       1.00      1.00      1.00       100\n",
      "                Pepper,_bell___Bacterial_spot       1.00      1.00      1.00       100\n",
      "                       Pepper,_bell___healthy       1.00      1.00      1.00       148\n",
      "                        Potato___Early_blight       1.00      1.00      1.00       100\n",
      "                         Potato___Late_blight       1.00      1.00      1.00       100\n",
      "                             Potato___healthy       1.00      1.00      1.00       100\n",
      "                          Raspberry___healthy       1.00      1.00      1.00       100\n",
      "                            Soybean___healthy       1.00      1.00      1.00       509\n",
      "                      Squash___Powdery_mildew       1.00      1.00      1.00       184\n",
      "                     Strawberry___Leaf_scorch       1.00      1.00      1.00       111\n",
      "                         Strawberry___healthy       1.00      1.00      1.00       100\n",
      "                      Tomato___Bacterial_spot       1.00      1.00      1.00       213\n",
      "                        Tomato___Early_blight       1.00      1.00      1.00       100\n",
      "                         Tomato___Late_blight       1.00      0.99      1.00       191\n",
      "                           Tomato___Leaf_Mold       1.00      1.00      1.00       100\n",
      "                  Tomato___Septoria_leaf_spot       0.99      1.00      1.00       177\n",
      "Tomato___Spider_mites Two-spotted_spider_mite       1.00      1.00      1.00       168\n",
      "                         Tomato___Target_Spot       0.99      0.99      0.99       140\n",
      "       Tomato___Tomato_Yellow_Leaf_Curl_Virus       1.00      1.00      1.00       536\n",
      "                 Tomato___Tomato_mosaic_virus       0.99      1.00      1.00       100\n",
      "                             Tomato___healthy       1.00      0.99      1.00       159\n",
      "\n",
      "                                     accuracy                           1.00      6149\n",
      "                                    macro avg       1.00      1.00      1.00      6149\n",
      "                                 weighted avg       1.00      1.00      1.00      6149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rapport complet\n",
    "print(\"=== Test Set Evaluation (Student) ===\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

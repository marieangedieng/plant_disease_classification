{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.7.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.3.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.3)\n",
      "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.11/dist-packages (4.9.9)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.1.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (77.0.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: array_record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.7.2)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.1.9)\n",
      "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.13.0)\n",
      "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (4.2.1)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (7.0.0)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (21.0.0)\n",
      "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.1.7)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (1.17.2)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (4.67.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (0.8.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2024.10.0)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.5.2)\n",
      "Requirement already satisfied: zipp in /usr/lib/python3/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.0.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow_datasets) (25.3.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple_parsing->tensorflow_datasets) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.70.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U opencv-python tensorflow scikit-learn pandas matplotlib tensorflow_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTATION DES LIBRAIRIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 00:45:31.537465: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753058731.551766    2431 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753058731.556384    2431 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753058731.569094    2431 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753058731.569107    2431 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753058731.569108    2431 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753058731.569110    2431 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-21 00:45:31.572998: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets.public_api as tfds\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def telecharger_dezip(url, chemin_sauv=\"plant_village_dataset.zip\", extract_path=\".\"):\n",
    "    print(\" Début du téléchargement\")\n",
    "    try:\n",
    "        response=requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        #Taille totale du fichier pour la barre de progression\n",
    "        total_size=int(response.headers.get('content-length',0))\n",
    "        block_size=1064\n",
    "        bar_progression = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "\n",
    "        #Téléchargement\n",
    "        with open(chemin_sauv, 'wb') as file:\n",
    "            for data in response.iter_content(block_size):\n",
    "                bar_progression.update(len(data))\n",
    "                file.write(data)\n",
    "        bar_progression.close()\n",
    "\n",
    "        if total_size != 0 and bar_progression.n != total_size:\n",
    "            print(\"ERREUR, quelque chose s'est mal passé pendant le téléchargement.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Téléchargement terminé. Fichier sauvegardé sous : {chemin_sauv}\")\n",
    "\n",
    "        # Créer le dossier d'extraction s'il n'existe pas\n",
    "        if not os.path.exists(extract_path):\n",
    "            os.makedirs(extract_path)\n",
    "\n",
    "        # Décompresser le fichier ZIP\n",
    "        print(f\"Décompression du fichier dans le dossier : {extract_path}\")\n",
    "        with zipfile.ZipFile(chemin_sauv, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "\n",
    "        print(\"Décompression terminée.\")\n",
    "\n",
    "        # Optionnel : Supprimer le fichier .zip après extraction pour économiser de l'espace\n",
    "        print(f\"Suppression du fichier {chemin_sauv}...\")\n",
    "        os.remove(chemin_sauv)\n",
    "        print(\"Opération terminée avec succès !\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Une erreur de réseau est survenue: {e}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"Erreur: Le fichier téléchargé n'est pas un fichier ZIP valide.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur inattendue est survenue: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://data.mendeley.com/datasets/tywbtsjrjv/1/files/b4e3a32f-c0bd-4060-81e9-6144231f2520/file_downloaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_folder = \"plant_village_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Début du téléchargement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 949M/949M [00:44<00:00, 21.3MiB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement terminé. Fichier sauvegardé sous : PlantVillage.zip\n",
      "Décompression du fichier dans le dossier : plant_village_dataset\n",
      "Décompression terminée.\n",
      "Suppression du fichier PlantVillage.zip...\n",
      "Opération terminée avec succès !\n"
     ]
    }
   ],
   "source": [
    "telecharger_dezip(URL, \"PlantVillage.zip\", extract_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/workspace/plant_village_dataset/Plant_leave_diseases_dataset_with_augmentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61486 images belonging to 39 classes.\n"
     ]
    }
   ],
   "source": [
    "data=data_gen.flow_from_directory(\n",
    "    path,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResTS avec \n",
    "\n",
    "**Teacher:resnet50**\n",
    "\n",
    "**Student:resnet18**\n",
    "\n",
    "P, F. R. P., U, A. S., Moustafa, M. A., & Ali, M. A. S. (2023). Detecting plant disease in corn leaf using EfficientNet Architecture—An analytical approach. Electronics, 12(8), 1938. https://doi.org/10.3390/electronics12081938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total des images trouvées : 61486\n"
     ]
    }
   ],
   "source": [
    "# --------- 1. Préparer les données ---------\n",
    "filepaths = []\n",
    "labels = []\n",
    "folds = os.listdir(path)\n",
    "for fold in folds:\n",
    "    f_path = os.path.join(path, fold)\n",
    "    if not os.path.isdir(f_path):\n",
    "        continue\n",
    "    for file in os.listdir(f_path):\n",
    "        filepaths.append(os.path.join(f_path, file))\n",
    "        labels.append(fold)\n",
    "\n",
    "df = pd.DataFrame({'filepaths': filepaths, 'labels': labels})\n",
    "print(f\"Total des images trouvées : {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 80/20 avec stratification\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=df['labels']\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df['labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping des classes en indices\n",
    "class_names = sorted(df['labels'].unique())\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- 2. Dataset personnalisé ---------\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, class_to_idx, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.loc[idx, 'filepaths']\n",
    "        label_name = self.df.loc[idx, 'labels']\n",
    "        label = self.class_to_idx[label_name]\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- 3. Data augmentation et loaders ---------\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]) # EfficientNet normalization\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomImageDataset(train_df, class_to_idx, transform=train_transforms)\n",
    "val_dataset = CustomImageDataset(val_df, class_to_idx, transform=val_transforms)\n",
    "test_dataset = CustomImageDataset(test_df, class_to_idx, transform=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 273MB/s]\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 437MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=39, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- Teacher & Student Models ----------\n",
    "teacher = models.resnet50(pretrained=True)\n",
    "teacher.fc = nn.Linear(teacher.fc.in_features, num_classes)\n",
    "teacher.to(device)\n",
    "teacher.eval()  # Le teacher ne sera pas entraîné\n",
    "\n",
    "student = models.resnet18(pretrained=True)\n",
    "student.fc = nn.Linear(student.fc.in_features, num_classes)\n",
    "student.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Distillation Loss ----------\n",
    "def distillation_loss(student_logits, teacher_logits, labels, alpha=0.5, temperature=3.0):\n",
    "    \"\"\"\n",
    "    alpha: poids entre CrossEntropy (vraies étiquettes) et KLDiv (distillation)\n",
    "    temperature: adoucit les probabilités du teacher\n",
    "    \"\"\"\n",
    "    ce_loss = F.cross_entropy(student_logits, labels)\n",
    "    # KL divergence entre distributions adoucies\n",
    "    p_student = F.log_softmax(student_logits / temperature, dim=1)\n",
    "    p_teacher = F.softmax(teacher_logits / temperature, dim=1)\n",
    "    kl_loss = F.kl_div(p_student, p_teacher, reduction=\"batchmean\") * (temperature ** 2)\n",
    "    return alpha * ce_loss + (1 - alpha) * kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(student.parameters(), lr=0.001)\n",
    "epochs = 20\n",
    "patience = 5\n",
    "best_val_acc = 0.0\n",
    "epochs_no_improve = 0\n",
    "best_model_path = \"/workspace/models/best_model_resTS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Train Loss: 0.7861, Train Acc: 0.8039 - Val Loss: 0.5018, Val Acc: 0.9493\n",
      ">> Nouveau meilleur Student sauvegardé.\n",
      "Epoch [2/20] - Train Loss: 0.6084, Train Acc: 0.9029 - Val Loss: 0.4726, Val Acc: 0.9434\n",
      "Epoch [3/20] - Train Loss: 0.5686, Train Acc: 0.9220 - Val Loss: 0.5041, Val Acc: 0.9501\n",
      ">> Nouveau meilleur Student sauvegardé.\n",
      "Epoch [4/20] - Train Loss: 0.5401, Train Acc: 0.9350 - Val Loss: 0.4012, Val Acc: 0.9736\n",
      ">> Nouveau meilleur Student sauvegardé.\n",
      "Epoch [5/20] - Train Loss: 0.5264, Train Acc: 0.9412 - Val Loss: 0.3614, Val Acc: 0.9793\n",
      ">> Nouveau meilleur Student sauvegardé.\n",
      "Epoch [6/20] - Train Loss: 0.5118, Train Acc: 0.9466 - Val Loss: 0.3725, Val Acc: 0.9810\n",
      ">> Nouveau meilleur Student sauvegardé.\n",
      "Epoch [7/20] - Train Loss: 0.5032, Train Acc: 0.9488 - Val Loss: 0.3310, Val Acc: 0.9877\n",
      ">> Nouveau meilleur Student sauvegardé.\n",
      "Epoch [8/20] - Train Loss: 0.4933, Train Acc: 0.9549 - Val Loss: 0.3433, Val Acc: 0.9827\n",
      "Epoch [9/20] - Train Loss: 0.4861, Train Acc: 0.9576 - Val Loss: 0.3495, Val Acc: 0.9800\n",
      "Epoch [10/20] - Train Loss: 0.4819, Train Acc: 0.9588 - Val Loss: 0.3653, Val Acc: 0.9816\n",
      "Epoch [11/20] - Train Loss: 0.4765, Train Acc: 0.9598 - Val Loss: 0.3664, Val Acc: 0.9784\n",
      "Epoch [12/20] - Train Loss: 0.4697, Train Acc: 0.9640 - Val Loss: 0.3342, Val Acc: 0.9883\n",
      ">> Nouveau meilleur Student sauvegardé.\n",
      "Epoch [13/20] - Train Loss: 0.4678, Train Acc: 0.9651 - Val Loss: 0.3522, Val Acc: 0.9828\n",
      "Epoch [14/20] - Train Loss: 0.4651, Train Acc: 0.9648 - Val Loss: 0.3445, Val Acc: 0.9888\n",
      ">> Nouveau meilleur Student sauvegardé.\n",
      "Epoch [15/20] - Train Loss: 0.4621, Train Acc: 0.9664 - Val Loss: 0.3510, Val Acc: 0.9885\n",
      "Epoch [16/20] - Train Loss: 0.4580, Train Acc: 0.9682 - Val Loss: 0.3392, Val Acc: 0.9911\n",
      ">> Nouveau meilleur Student sauvegardé.\n",
      "Epoch [17/20] - Train Loss: 0.4559, Train Acc: 0.9702 - Val Loss: 0.3719, Val Acc: 0.9849\n",
      "Epoch [18/20] - Train Loss: 0.4550, Train Acc: 0.9707 - Val Loss: 0.3295, Val Acc: 0.9869\n",
      "Epoch [19/20] - Train Loss: 0.4519, Train Acc: 0.9709 - Val Loss: 0.3295, Val Acc: 0.9911\n",
      ">> Nouveau meilleur Student sauvegardé.\n",
      "Epoch [20/20] - Train Loss: 0.4513, Train Acc: 0.9714 - Val Loss: 0.3397, Val Acc: 0.9914\n",
      ">> Nouveau meilleur Student sauvegardé.\n",
      "Entraînement ResTS terminé. Meilleure accuracy Student (val) : 0.9914\n"
     ]
    }
   ],
   "source": [
    "# ---------- Entraînement ----------\n",
    "for epoch in range(epochs):\n",
    "    student.train()\n",
    "    running_loss, running_corrects = 0.0, 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(inputs)  # Sorties du teacher\n",
    "\n",
    "        student_outputs = student(inputs)\n",
    "        loss = distillation_loss(student_outputs, teacher_outputs, labels, alpha=0.5, temperature=3.0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        preds = torch.argmax(student_outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    train_acc = running_corrects / len(train_dataset)\n",
    "    \n",
    "    # Validation\n",
    "    student.eval()\n",
    "    val_loss, val_corrects = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = student(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            \n",
    "            preds = torch.argmax(outputs, 1)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_corrects += torch.sum(preds == labels).item()\n",
    "    \n",
    "    val_loss /= len(val_dataset)\n",
    "    val_acc = val_corrects / len(val_dataset)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(student.state_dict(), best_model_path)\n",
    "        print(\">> Nouveau meilleur Student sauvegardé.\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\">> Early stopping déclenché.\")\n",
    "            break\n",
    "\n",
    "print(f\"Entraînement ResTS terminé. Meilleure accuracy Student (val) : {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, psutil, torch\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=39, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Charger le Student entraîné (ResTS)\n",
    "student.load_state_dict(torch.load(\"/workspace/models/best_model_resTS\", map_location=device))\n",
    "student.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1] Time: 0.01s | CPU: 3.9% | RAM: 85.44GB | GPU: 299.57MB\n",
      "[Batch 2] Time: 0.01s | CPU: 3.1% | RAM: 85.45GB | GPU: 299.57MB\n",
      "[Batch 3] Time: 0.01s | CPU: 5.6% | RAM: 85.46GB | GPU: 299.57MB\n",
      "[Batch 4] Time: 0.01s | CPU: 6.0% | RAM: 85.47GB | GPU: 299.57MB\n",
      "[Batch 5] Time: 0.01s | CPU: 4.7% | RAM: 85.53GB | GPU: 299.57MB\n",
      "[Batch 6] Time: 0.01s | CPU: 4.5% | RAM: 85.55GB | GPU: 299.57MB\n",
      "[Batch 7] Time: 0.01s | CPU: 5.1% | RAM: 85.55GB | GPU: 299.57MB\n",
      "[Batch 8] Time: 0.01s | CPU: 5.6% | RAM: 85.57GB | GPU: 299.57MB\n",
      "[Batch 9] Time: 0.01s | CPU: 4.6% | RAM: 85.57GB | GPU: 299.57MB\n",
      "[Batch 10] Time: 0.01s | CPU: 4.9% | RAM: 85.57GB | GPU: 299.57MB\n",
      "[Batch 11] Time: 0.01s | CPU: 5.2% | RAM: 85.57GB | GPU: 299.57MB\n",
      "[Batch 12] Time: 0.01s | CPU: 5.2% | RAM: 85.58GB | GPU: 299.57MB\n",
      "[Batch 13] Time: 0.01s | CPU: 4.6% | RAM: 85.58GB | GPU: 299.57MB\n",
      "[Batch 14] Time: 0.01s | CPU: 4.5% | RAM: 85.59GB | GPU: 299.57MB\n",
      "[Batch 15] Time: 0.01s | CPU: 5.5% | RAM: 85.59GB | GPU: 299.57MB\n",
      "[Batch 16] Time: 0.01s | CPU: 5.3% | RAM: 85.60GB | GPU: 299.57MB\n",
      "[Batch 17] Time: 0.01s | CPU: 3.9% | RAM: 85.62GB | GPU: 299.57MB\n",
      "[Batch 18] Time: 0.01s | CPU: 6.3% | RAM: 85.63GB | GPU: 299.57MB\n",
      "[Batch 19] Time: 0.01s | CPU: 4.3% | RAM: 85.62GB | GPU: 299.57MB\n",
      "[Batch 20] Time: 0.01s | CPU: 5.1% | RAM: 85.62GB | GPU: 299.57MB\n",
      "[Batch 21] Time: 0.01s | CPU: 4.1% | RAM: 85.62GB | GPU: 299.57MB\n",
      "[Batch 22] Time: 0.01s | CPU: 5.1% | RAM: 85.61GB | GPU: 299.57MB\n",
      "[Batch 23] Time: 0.01s | CPU: 2.9% | RAM: 85.62GB | GPU: 299.57MB\n",
      "[Batch 24] Time: 0.01s | CPU: 4.6% | RAM: 85.62GB | GPU: 299.57MB\n",
      "[Batch 25] Time: 0.01s | CPU: 4.0% | RAM: 85.63GB | GPU: 299.57MB\n",
      "[Batch 26] Time: 0.01s | CPU: 4.1% | RAM: 85.63GB | GPU: 299.57MB\n",
      "[Batch 27] Time: 0.01s | CPU: 4.8% | RAM: 85.62GB | GPU: 299.57MB\n",
      "[Batch 28] Time: 0.01s | CPU: 5.8% | RAM: 85.62GB | GPU: 299.57MB\n",
      "[Batch 29] Time: 0.01s | CPU: 3.8% | RAM: 85.62GB | GPU: 299.57MB\n",
      "[Batch 30] Time: 0.01s | CPU: 3.5% | RAM: 85.62GB | GPU: 299.57MB\n",
      "[Batch 31] Time: 0.01s | CPU: 5.4% | RAM: 85.62GB | GPU: 299.57MB\n",
      "[Batch 32] Time: 0.01s | CPU: 4.0% | RAM: 85.63GB | GPU: 299.57MB\n",
      "[Batch 33] Time: 0.01s | CPU: 3.3% | RAM: 85.62GB | GPU: 299.57MB\n",
      "[Batch 34] Time: 0.01s | CPU: 4.1% | RAM: 85.63GB | GPU: 299.57MB\n",
      "[Batch 35] Time: 0.01s | CPU: 4.8% | RAM: 85.63GB | GPU: 299.57MB\n",
      "[Batch 36] Time: 0.01s | CPU: 4.6% | RAM: 85.63GB | GPU: 299.57MB\n",
      "[Batch 37] Time: 0.01s | CPU: 3.6% | RAM: 85.63GB | GPU: 299.57MB\n",
      "[Batch 38] Time: 0.01s | CPU: 4.4% | RAM: 85.63GB | GPU: 299.57MB\n",
      "[Batch 39] Time: 0.01s | CPU: 4.3% | RAM: 85.63GB | GPU: 299.57MB\n",
      "[Batch 40] Time: 0.01s | CPU: 4.7% | RAM: 85.64GB | GPU: 299.57MB\n",
      "[Batch 41] Time: 0.01s | CPU: 3.7% | RAM: 85.64GB | GPU: 299.57MB\n",
      "[Batch 42] Time: 0.01s | CPU: 5.2% | RAM: 85.63GB | GPU: 299.57MB\n",
      "[Batch 43] Time: 0.01s | CPU: 4.6% | RAM: 85.63GB | GPU: 299.57MB\n",
      "[Batch 44] Time: 0.01s | CPU: 4.8% | RAM: 85.63GB | GPU: 299.57MB\n",
      "[Batch 45] Time: 0.01s | CPU: 3.1% | RAM: 85.62GB | GPU: 299.57MB\n",
      "[Batch 46] Time: 0.01s | CPU: 4.3% | RAM: 85.62GB | GPU: 299.57MB\n",
      "[Batch 47] Time: 0.01s | CPU: 4.7% | RAM: 85.63GB | GPU: 299.57MB\n",
      "[Batch 48] Time: 0.01s | CPU: 5.2% | RAM: 85.63GB | GPU: 299.57MB\n",
      "[Batch 49] Time: 0.01s | CPU: 3.8% | RAM: 85.64GB | GPU: 299.57MB\n",
      "[Batch 50] Time: 0.01s | CPU: 4.9% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 51] Time: 0.01s | CPU: 5.2% | RAM: 85.64GB | GPU: 299.57MB\n",
      "[Batch 52] Time: 0.01s | CPU: 5.7% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 53] Time: 0.01s | CPU: 4.0% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 54] Time: 0.01s | CPU: 2.9% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 55] Time: 0.01s | CPU: 5.0% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 56] Time: 0.01s | CPU: 5.7% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 57] Time: 0.01s | CPU: 4.4% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 58] Time: 0.01s | CPU: 5.7% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 59] Time: 0.01s | CPU: 4.6% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 60] Time: 0.01s | CPU: 5.6% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 61] Time: 0.01s | CPU: 4.4% | RAM: 85.68GB | GPU: 299.57MB\n",
      "[Batch 62] Time: 0.01s | CPU: 5.6% | RAM: 85.68GB | GPU: 299.57MB\n",
      "[Batch 63] Time: 0.01s | CPU: 4.5% | RAM: 85.68GB | GPU: 299.57MB\n",
      "[Batch 64] Time: 0.01s | CPU: 4.8% | RAM: 85.68GB | GPU: 299.57MB\n",
      "[Batch 65] Time: 0.01s | CPU: 4.1% | RAM: 85.70GB | GPU: 299.57MB\n",
      "[Batch 66] Time: 0.01s | CPU: 4.1% | RAM: 85.69GB | GPU: 299.57MB\n",
      "[Batch 67] Time: 0.01s | CPU: 4.8% | RAM: 85.69GB | GPU: 299.57MB\n",
      "[Batch 68] Time: 0.01s | CPU: 4.5% | RAM: 85.69GB | GPU: 299.57MB\n",
      "[Batch 69] Time: 0.01s | CPU: 4.1% | RAM: 85.68GB | GPU: 299.57MB\n",
      "[Batch 70] Time: 0.01s | CPU: 5.1% | RAM: 85.68GB | GPU: 299.57MB\n",
      "[Batch 71] Time: 0.01s | CPU: 4.9% | RAM: 85.68GB | GPU: 299.57MB\n",
      "[Batch 72] Time: 0.01s | CPU: 5.7% | RAM: 85.69GB | GPU: 299.57MB\n",
      "[Batch 73] Time: 0.01s | CPU: 4.5% | RAM: 85.68GB | GPU: 299.57MB\n",
      "[Batch 74] Time: 0.01s | CPU: 5.3% | RAM: 85.68GB | GPU: 299.57MB\n",
      "[Batch 75] Time: 0.01s | CPU: 5.4% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 76] Time: 0.01s | CPU: 6.2% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 77] Time: 0.01s | CPU: 4.1% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 78] Time: 0.01s | CPU: 5.5% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 79] Time: 0.01s | CPU: 5.3% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 80] Time: 0.01s | CPU: 4.7% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 81] Time: 0.01s | CPU: 4.4% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 82] Time: 0.01s | CPU: 5.2% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 83] Time: 0.01s | CPU: 5.5% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 84] Time: 0.01s | CPU: 4.9% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 85] Time: 0.01s | CPU: 4.1% | RAM: 85.68GB | GPU: 299.57MB\n",
      "[Batch 86] Time: 0.01s | CPU: 3.5% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 87] Time: 0.01s | CPU: 4.6% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 88] Time: 0.01s | CPU: 4.5% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 89] Time: 0.01s | CPU: 4.3% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 90] Time: 0.01s | CPU: 5.1% | RAM: 85.68GB | GPU: 299.57MB\n",
      "[Batch 91] Time: 0.01s | CPU: 5.5% | RAM: 85.68GB | GPU: 299.57MB\n",
      "[Batch 92] Time: 0.01s | CPU: 4.6% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 93] Time: 0.01s | CPU: 4.0% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 94] Time: 0.01s | CPU: 4.8% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 95] Time: 0.01s | CPU: 5.3% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 96] Time: 0.01s | CPU: 5.7% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 97] Time: 0.01s | CPU: 4.2% | RAM: 85.68GB | GPU: 299.57MB\n",
      "[Batch 98] Time: 0.01s | CPU: 5.5% | RAM: 85.68GB | GPU: 299.57MB\n",
      "[Batch 99] Time: 0.01s | CPU: 4.8% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 100] Time: 0.01s | CPU: 5.0% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 101] Time: 0.01s | CPU: 4.4% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 102] Time: 0.01s | CPU: 5.2% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 103] Time: 0.01s | CPU: 4.8% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 104] Time: 0.01s | CPU: 4.8% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 105] Time: 0.01s | CPU: 4.4% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 106] Time: 0.01s | CPU: 5.1% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 107] Time: 0.01s | CPU: 5.5% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 108] Time: 0.01s | CPU: 5.1% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 109] Time: 0.01s | CPU: 4.4% | RAM: 85.68GB | GPU: 299.57MB\n",
      "[Batch 110] Time: 0.01s | CPU: 4.6% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 111] Time: 0.01s | CPU: 5.9% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 112] Time: 0.01s | CPU: 5.1% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 113] Time: 0.01s | CPU: 4.4% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 114] Time: 0.01s | CPU: 5.6% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 115] Time: 0.01s | CPU: 4.8% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 116] Time: 0.01s | CPU: 5.1% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 117] Time: 0.01s | CPU: 4.0% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 118] Time: 0.01s | CPU: 5.1% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 119] Time: 0.01s | CPU: 4.5% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 120] Time: 0.01s | CPU: 4.5% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 121] Time: 0.01s | CPU: 4.6% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 122] Time: 0.01s | CPU: 5.2% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 123] Time: 0.01s | CPU: 5.1% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 124] Time: 0.01s | CPU: 5.2% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 125] Time: 0.01s | CPU: 5.2% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 126] Time: 0.01s | CPU: 5.7% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 127] Time: 0.01s | CPU: 6.3% | RAM: 85.64GB | GPU: 299.57MB\n",
      "[Batch 128] Time: 0.01s | CPU: 6.1% | RAM: 85.64GB | GPU: 299.57MB\n",
      "[Batch 129] Time: 0.01s | CPU: 5.2% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 130] Time: 0.01s | CPU: 5.4% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 131] Time: 0.01s | CPU: 14.5% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 132] Time: 0.01s | CPU: 6.7% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 133] Time: 0.01s | CPU: 5.3% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 134] Time: 0.01s | CPU: 3.9% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 135] Time: 0.01s | CPU: 4.6% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 136] Time: 0.01s | CPU: 5.7% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 137] Time: 0.01s | CPU: 4.2% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 138] Time: 0.01s | CPU: 5.0% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 139] Time: 0.01s | CPU: 4.9% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 140] Time: 0.01s | CPU: 5.1% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 141] Time: 0.01s | CPU: 4.6% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 142] Time: 0.01s | CPU: 2.8% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 143] Time: 0.01s | CPU: 2.4% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 144] Time: 0.01s | CPU: 1.7% | RAM: 85.65GB | GPU: 299.57MB\n",
      "[Batch 145] Time: 0.01s | CPU: 4.3% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 146] Time: 0.01s | CPU: 5.3% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 147] Time: 0.01s | CPU: 5.5% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 148] Time: 0.01s | CPU: 3.8% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 149] Time: 0.01s | CPU: 4.6% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 150] Time: 0.01s | CPU: 5.6% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 151] Time: 0.01s | CPU: 5.4% | RAM: 85.68GB | GPU: 299.57MB\n",
      "[Batch 152] Time: 0.01s | CPU: 4.9% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 153] Time: 0.01s | CPU: 4.4% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 154] Time: 0.01s | CPU: 4.7% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 155] Time: 0.01s | CPU: 4.8% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 156] Time: 0.01s | CPU: 5.5% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 157] Time: 0.01s | CPU: 4.7% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 158] Time: 0.01s | CPU: 4.9% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 159] Time: 0.01s | CPU: 4.8% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 160] Time: 0.01s | CPU: 5.0% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 161] Time: 0.01s | CPU: 4.6% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 162] Time: 0.01s | CPU: 4.7% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 163] Time: 0.01s | CPU: 5.4% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 164] Time: 0.01s | CPU: 5.0% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 165] Time: 0.01s | CPU: 3.9% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 166] Time: 0.01s | CPU: 5.5% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 167] Time: 0.01s | CPU: 5.0% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 168] Time: 0.01s | CPU: 5.2% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 169] Time: 0.01s | CPU: 4.3% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 170] Time: 0.01s | CPU: 4.1% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 171] Time: 0.01s | CPU: 5.4% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 172] Time: 0.01s | CPU: 6.0% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 173] Time: 0.01s | CPU: 4.3% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 174] Time: 0.01s | CPU: 5.6% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 175] Time: 0.01s | CPU: 5.2% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 176] Time: 0.01s | CPU: 4.6% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 177] Time: 0.01s | CPU: 3.6% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 178] Time: 0.01s | CPU: 5.5% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 179] Time: 0.01s | CPU: 5.4% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 180] Time: 0.01s | CPU: 5.4% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 181] Time: 0.01s | CPU: 4.6% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 182] Time: 0.01s | CPU: 5.2% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 183] Time: 0.01s | CPU: 5.0% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 184] Time: 0.01s | CPU: 3.9% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 185] Time: 0.01s | CPU: 4.2% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 186] Time: 0.01s | CPU: 5.7% | RAM: 85.66GB | GPU: 299.57MB\n",
      "[Batch 187] Time: 0.01s | CPU: 4.7% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 188] Time: 0.01s | CPU: 2.7% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 189] Time: 0.01s | CPU: 2.7% | RAM: 85.68GB | GPU: 299.57MB\n",
      "[Batch 190] Time: 0.01s | CPU: 3.5% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 191] Time: 0.01s | CPU: 3.4% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 192] Time: 0.01s | CPU: 3.0% | RAM: 85.67GB | GPU: 299.57MB\n",
      "[Batch 193] Time: 0.00s | CPU: 2.2% | RAM: 85.67GB | GPU: 284.06MB\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "        batch_start = time.time()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = student(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # --- Profiling ---\n",
    "        cpu_usage = psutil.cpu_percent(interval=None)\n",
    "        ram = psutil.virtual_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "        else:\n",
    "            gpu_mem = 0.0\n",
    "        print(f\"[Batch {i+1}] Time: {time.time()-batch_start:.2f}s | CPU: {cpu_usage:.1f}% | RAM: {ram.used/1024**3:.2f}GB | GPU: {gpu_mem:.2f}MB\")\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temps Test Total: 8.56 sec\n",
      "Throughput: 718.44 images/sec\n"
     ]
    }
   ],
   "source": [
    "total_time = end_time - start_time\n",
    "print(f\"\\nTemps Test Total: {total_time:.2f} sec\")\n",
    "print(f\"Throughput: {len(test_dataset) / total_time:.2f} images/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Set Evaluation (Student) ===\n",
      "                                               precision    recall  f1-score   support\n",
      "\n",
      "                           Apple___Apple_scab       1.00      0.97      0.98       100\n",
      "                            Apple___Black_rot       0.99      1.00      1.00       100\n",
      "                     Apple___Cedar_apple_rust       1.00      1.00      1.00       100\n",
      "                              Apple___healthy       0.98      1.00      0.99       164\n",
      "                    Background_without_leaves       0.97      0.99      0.98       114\n",
      "                          Blueberry___healthy       1.00      1.00      1.00       150\n",
      "                      Cherry___Powdery_mildew       1.00      1.00      1.00       105\n",
      "                             Cherry___healthy       1.00      0.99      0.99       100\n",
      "   Corn___Cercospora_leaf_spot Gray_leaf_spot       0.95      0.99      0.97       100\n",
      "                           Corn___Common_rust       1.00      1.00      1.00       119\n",
      "                  Corn___Northern_Leaf_Blight       0.99      0.95      0.97       100\n",
      "                               Corn___healthy       1.00      1.00      1.00       116\n",
      "                            Grape___Black_rot       1.00      0.99      1.00       118\n",
      "                 Grape___Esca_(Black_Measles)       0.99      1.00      1.00       138\n",
      "   Grape___Leaf_blight_(Isariopsis_Leaf_Spot)       1.00      1.00      1.00       108\n",
      "                              Grape___healthy       0.99      1.00      1.00       100\n",
      "     Orange___Haunglongbing_(Citrus_greening)       1.00      1.00      1.00       551\n",
      "                       Peach___Bacterial_spot       1.00      1.00      1.00       230\n",
      "                              Peach___healthy       1.00      1.00      1.00       100\n",
      "                Pepper,_bell___Bacterial_spot       0.99      1.00      1.00       100\n",
      "                       Pepper,_bell___healthy       1.00      1.00      1.00       148\n",
      "                        Potato___Early_blight       1.00      0.97      0.98       100\n",
      "                         Potato___Late_blight       0.99      1.00      1.00       100\n",
      "                             Potato___healthy       1.00      1.00      1.00       100\n",
      "                          Raspberry___healthy       1.00      1.00      1.00       100\n",
      "                            Soybean___healthy       1.00      0.99      1.00       509\n",
      "                      Squash___Powdery_mildew       1.00      1.00      1.00       184\n",
      "                     Strawberry___Leaf_scorch       1.00      0.99      1.00       111\n",
      "                         Strawberry___healthy       1.00      1.00      1.00       100\n",
      "                      Tomato___Bacterial_spot       1.00      1.00      1.00       213\n",
      "                        Tomato___Early_blight       0.98      0.91      0.94       100\n",
      "                         Tomato___Late_blight       0.97      0.99      0.98       191\n",
      "                           Tomato___Leaf_Mold       1.00      1.00      1.00       100\n",
      "                  Tomato___Septoria_leaf_spot       0.99      0.99      0.99       177\n",
      "Tomato___Spider_mites Two-spotted_spider_mite       0.97      1.00      0.99       168\n",
      "                         Tomato___Target_Spot       0.95      0.95      0.95       140\n",
      "       Tomato___Tomato_Yellow_Leaf_Curl_Virus       1.00      1.00      1.00       536\n",
      "                 Tomato___Tomato_mosaic_virus       0.99      1.00      1.00       100\n",
      "                             Tomato___healthy       1.00      0.99      1.00       159\n",
      "\n",
      "                                     accuracy                           0.99      6149\n",
      "                                    macro avg       0.99      0.99      0.99      6149\n",
      "                                 weighted avg       0.99      0.99      0.99      6149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rapport complet\n",
    "print(\"=== Test Set Evaluation (Student) ===\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

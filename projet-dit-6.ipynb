{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.20.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.7.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.3.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.5)\n",
      "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.11/dist-packages (4.9.9)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.1.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (6.31.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (77.0.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.11.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: array_record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.7.2)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.1.9)\n",
      "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.13.0)\n",
      "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (4.2.1)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (7.0.0)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (21.0.0)\n",
      "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.1.7)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (1.17.2)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (4.67.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (0.8.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2024.10.0)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.5.2)\n",
      "Requirement already satisfied: zipp in /usr/lib/python3/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.0.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.10.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow_datasets) (25.3.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple_parsing->tensorflow_datasets) (0.17.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.70.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U opencv-python tensorflow scikit-learn pandas matplotlib tensorflow_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTATION DES LIBRAIRIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 11:22:38.818877: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets.public_api as tfds\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def telecharger_dezip(url, chemin_sauv=\"plant_village_dataset.zip\", extract_path=\".\"):\n",
    "    print(\" Début du téléchargement\")\n",
    "    try:\n",
    "        response=requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        #Taille totale du fichier pour la barre de progression\n",
    "        total_size=int(response.headers.get('content-length',0))\n",
    "        block_size=1064\n",
    "        bar_progression = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "\n",
    "        #Téléchargement\n",
    "        with open(chemin_sauv, 'wb') as file:\n",
    "            for data in response.iter_content(block_size):\n",
    "                bar_progression.update(len(data))\n",
    "                file.write(data)\n",
    "        bar_progression.close()\n",
    "\n",
    "        if total_size != 0 and bar_progression.n != total_size:\n",
    "            print(\"ERREUR, quelque chose s'est mal passé pendant le téléchargement.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Téléchargement terminé. Fichier sauvegardé sous : {chemin_sauv}\")\n",
    "\n",
    "        # Créer le dossier d'extraction s'il n'existe pas\n",
    "        if not os.path.exists(extract_path):\n",
    "            os.makedirs(extract_path)\n",
    "\n",
    "        # Décompresser le fichier ZIP\n",
    "        print(f\"Décompression du fichier dans le dossier : {extract_path}\")\n",
    "        with zipfile.ZipFile(chemin_sauv, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "\n",
    "        print(\"Décompression terminée.\")\n",
    "\n",
    "        # Optionnel : Supprimer le fichier .zip après extraction pour économiser de l'espace\n",
    "        print(f\"Suppression du fichier {chemin_sauv}...\")\n",
    "        os.remove(chemin_sauv)\n",
    "        print(\"Opération terminée avec succès !\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Une erreur de réseau est survenue: {e}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"Erreur: Le fichier téléchargé n'est pas un fichier ZIP valide.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur inattendue est survenue: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://data.mendeley.com/datasets/tywbtsjrjv/1/files/b4e3a32f-c0bd-4060-81e9-6144231f2520/file_downloaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_folder = \"plant_village_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Début du téléchargement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 949M/949M [00:44<00:00, 21.3MiB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement terminé. Fichier sauvegardé sous : PlantVillage.zip\n",
      "Décompression du fichier dans le dossier : plant_village_dataset\n",
      "Décompression terminée.\n",
      "Suppression du fichier PlantVillage.zip...\n",
      "Opération terminée avec succès !\n"
     ]
    }
   ],
   "source": [
    "telecharger_dezip(URL, \"PlantVillage.zip\", extract_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/workspace/plant_village_dataset/Plant_leave_diseases_dataset_with_augmentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "TEMPERATURE = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61486 images belonging to 39 classes.\n"
     ]
    }
   ],
   "source": [
    "data=data_gen.flow_from_directory(\n",
    "    path,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TS avec \n",
    "\n",
    "**Teacher:resnet50**\n",
    "\n",
    "**Student:mobilenetv3-small**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total des images trouvées : 61486\n"
     ]
    }
   ],
   "source": [
    "# --------- 1. Préparer les données ---------\n",
    "filepaths = []\n",
    "labels = []\n",
    "folds = os.listdir(path)\n",
    "for fold in folds:\n",
    "    f_path = os.path.join(path, fold)\n",
    "    if not os.path.isdir(f_path):\n",
    "        continue\n",
    "    for file in os.listdir(f_path):\n",
    "        filepaths.append(os.path.join(f_path, file))\n",
    "        labels.append(fold)\n",
    "\n",
    "df = pd.DataFrame({'filepaths': filepaths, 'labels': labels})\n",
    "print(f\"Total des images trouvées : {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 80/20 avec stratification\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=df['labels']\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df['labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping des classes en indices\n",
    "class_names = sorted(df['labels'].unique())\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- 2. Dataset personnalisé ---------\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, class_to_idx, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.loc[idx, 'filepaths']\n",
    "        label_name = self.df.loc[idx, 'labels']\n",
    "        label = self.class_to_idx[label_name]\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- 3. Data augmentation et loaders ---------\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]) # EfficientNet normalization\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomImageDataset(train_df, class_to_idx, transform=train_transforms)\n",
    "val_dataset = CustomImageDataset(val_df, class_to_idx, transform=val_transforms)\n",
    "test_dataset = CustomImageDataset(test_df, class_to_idx, transform=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 563MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.83M/9.83M [00:00<00:00, 510MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Modèle Teacher (Professeur) - ResNet50\n",
    "teacher_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs_teacher = teacher_model.fc.in_features\n",
    "teacher_model.fc = nn.Linear(num_ftrs_teacher, num_classes)\n",
    "teacher_model = teacher_model.to(device)\n",
    "\n",
    "# Modèle Student (Étudiant) - MobileNetV3-small\n",
    "student_model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "num_ftrs_student = student_model.classifier[3].in_features\n",
    "student_model.classifier[3] = nn.Linear(num_ftrs_student, num_classes)\n",
    "student_model = student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_hard = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Distillation Loss ----------\n",
    "def distillation_loss(student_outputs, teacher_outputs, temperature, device):\n",
    "    teacher_outputs = teacher_outputs.detach()\n",
    "    soft_teacher_outputs = nn.functional.softmax(teacher_outputs / temperature, dim=1)\n",
    "    soft_student_outputs = nn.functional.log_softmax(student_outputs / temperature, dim=1)\n",
    "    dist_loss = nn.functional.kl_div(soft_student_outputs, soft_teacher_outputs, reduction='batchmean')\n",
    "    return dist_loss * (temperature * temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_student = optim.Adam(student_model.parameters(), lr=0.001)\n",
    "scheduler_student = optim.lr_scheduler.StepLR(optimizer_student, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_teacher_student(teacher_model, student_model, criterion_hard, optimizer_student, scheduler_student, num_epochs, train_loader, val_loader, device):\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_path = '/workspace/models/best_student_mobilenetv3_model.pth'\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        student_model.train()\n",
    "        teacher_model.eval()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer_student.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(inputs)\n",
    "            \n",
    "            student_outputs = student_model(inputs)\n",
    "            \n",
    "            hard_loss = criterion_hard(student_outputs, labels)\n",
    "            soft_loss = distillation_loss(student_outputs, teacher_outputs, TEMPERATURE, device)\n",
    "            \n",
    "            loss = hard_loss * 0.5 + soft_loss * 0.5\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer_student.step()\n",
    "            \n",
    "            _, preds = torch.max(student_outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        scheduler_student.step()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        student_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = student_model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion_hard(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} | '\n",
    "              f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "        \n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            torch.save(student_model.state_dict(), best_model_path)\n",
    "            print(f'Meilleur modèle étudiant sauvegardé avec une précision de validation de {best_val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/20: 100%|██████████| 1384/1384 [01:04<00:00, 21.29it/s]\n",
      "Validation Epoch 1/20: 100%|██████████| 346/346 [00:13<00:00, 26.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 0.6639 Acc: 0.9200 | Val Loss: 0.5540 Acc: 0.9752\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/20: 100%|██████████| 1384/1384 [01:07<00:00, 20.63it/s]\n",
      "Validation Epoch 2/20: 100%|██████████| 346/346 [00:13<00:00, 25.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Train Loss: 0.5898 Acc: 0.9491 | Val Loss: 0.6417 Acc: 0.9651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/20: 100%|██████████| 1384/1384 [01:06<00:00, 20.73it/s]\n",
      "Validation Epoch 3/20: 100%|██████████| 346/346 [00:12<00:00, 27.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Train Loss: 0.5675 Acc: 0.9538 | Val Loss: 0.5892 Acc: 0.9684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/20: 100%|██████████| 1384/1384 [01:05<00:00, 21.26it/s]\n",
      "Validation Epoch 4/20: 100%|██████████| 346/346 [00:13<00:00, 26.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Train Loss: 0.5511 Acc: 0.9597 | Val Loss: 0.4896 Acc: 0.9765\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/20: 100%|██████████| 1384/1384 [00:58<00:00, 23.55it/s]\n",
      "Validation Epoch 5/20: 100%|██████████| 346/346 [00:10<00:00, 33.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Train Loss: 0.5470 Acc: 0.9586 | Val Loss: 0.4681 Acc: 0.9835\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/20: 100%|██████████| 1384/1384 [00:59<00:00, 23.20it/s]\n",
      "Validation Epoch 6/20: 100%|██████████| 346/346 [00:13<00:00, 26.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Train Loss: 0.5390 Acc: 0.9627 | Val Loss: 0.6027 Acc: 0.9457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/20: 100%|██████████| 1384/1384 [01:01<00:00, 22.40it/s]\n",
      "Validation Epoch 7/20: 100%|██████████| 346/346 [00:12<00:00, 27.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Train Loss: 0.5332 Acc: 0.9633 | Val Loss: 0.4352 Acc: 0.9845\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/20: 100%|██████████| 1384/1384 [01:05<00:00, 21.14it/s]\n",
      "Validation Epoch 8/20: 100%|██████████| 346/346 [00:12<00:00, 27.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Train Loss: 0.5017 Acc: 0.9778 | Val Loss: 0.4149 Acc: 0.9933\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/20: 100%|██████████| 1384/1384 [01:03<00:00, 21.66it/s]\n",
      "Validation Epoch 9/20: 100%|██████████| 346/346 [00:12<00:00, 28.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Train Loss: 0.4947 Acc: 0.9813 | Val Loss: 0.4120 Acc: 0.9948\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/20: 100%|██████████| 1384/1384 [01:06<00:00, 20.75it/s]\n",
      "Validation Epoch 10/20: 100%|██████████| 346/346 [00:14<00:00, 24.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Train Loss: 0.4910 Acc: 0.9835 | Val Loss: 0.4022 Acc: 0.9951\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/20: 100%|██████████| 1384/1384 [01:09<00:00, 19.79it/s]\n",
      "Validation Epoch 11/20: 100%|██████████| 346/346 [00:12<00:00, 28.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Train Loss: 0.4880 Acc: 0.9846 | Val Loss: 0.3960 Acc: 0.9961\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/20: 100%|██████████| 1384/1384 [01:04<00:00, 21.48it/s]\n",
      "Validation Epoch 12/20: 100%|██████████| 346/346 [00:12<00:00, 27.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Train Loss: 0.4856 Acc: 0.9859 | Val Loss: 0.4082 Acc: 0.9955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/20: 100%|██████████| 1384/1384 [01:02<00:00, 21.97it/s]\n",
      "Validation Epoch 13/20: 100%|██████████| 346/346 [00:11<00:00, 29.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Train Loss: 0.4860 Acc: 0.9855 | Val Loss: 0.4053 Acc: 0.9958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/20: 100%|██████████| 1384/1384 [01:01<00:00, 22.49it/s]\n",
      "Validation Epoch 14/20: 100%|██████████| 346/346 [00:13<00:00, 25.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Train Loss: 0.4839 Acc: 0.9869 | Val Loss: 0.4023 Acc: 0.9963\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/20: 100%|██████████| 1384/1384 [01:04<00:00, 21.37it/s]\n",
      "Validation Epoch 15/20: 100%|██████████| 346/346 [00:12<00:00, 27.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Train Loss: 0.4831 Acc: 0.9868 | Val Loss: 0.4004 Acc: 0.9965\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/20: 100%|██████████| 1384/1384 [01:05<00:00, 21.18it/s]\n",
      "Validation Epoch 16/20: 100%|██████████| 346/346 [00:12<00:00, 28.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Train Loss: 0.4813 Acc: 0.9878 | Val Loss: 0.3999 Acc: 0.9969\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/20: 100%|██████████| 1384/1384 [01:00<00:00, 22.96it/s]\n",
      "Validation Epoch 17/20: 100%|██████████| 346/346 [00:10<00:00, 32.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Train Loss: 0.4813 Acc: 0.9876 | Val Loss: 0.3993 Acc: 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/20: 100%|██████████| 1384/1384 [00:56<00:00, 24.33it/s]\n",
      "Validation Epoch 18/20: 100%|██████████| 346/346 [00:10<00:00, 32.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Train Loss: 0.4806 Acc: 0.9884 | Val Loss: 0.4021 Acc: 0.9970\n",
      "Meilleur modèle étudiant sauvegardé avec une précision de validation de 0.9970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/20: 100%|██████████| 1384/1384 [01:02<00:00, 21.99it/s]\n",
      "Validation Epoch 19/20: 100%|██████████| 346/346 [00:13<00:00, 25.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Train Loss: 0.4812 Acc: 0.9879 | Val Loss: 0.4003 Acc: 0.9968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/20: 100%|██████████| 1384/1384 [01:05<00:00, 21.08it/s]\n",
      "Validation Epoch 20/20: 100%|██████████| 346/346 [00:13<00:00, 25.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Train Loss: 0.4810 Acc: 0.9881 | Val Loss: 0.3997 Acc: 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 20\n",
    "train_teacher_student(teacher_model, student_model, criterion_hard, optimizer_student, scheduler_student, NUM_EPOCHS, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, psutil, torch\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formation du modèle étudiant terminée. Chargement du meilleur modèle pour l'évaluation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileNetV3(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): Conv2dNormActivation(\n",
       "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
       "    (1): Hardswish()\n",
       "    (2): Dropout(p=0.2, inplace=True)\n",
       "    (3): Linear(in_features=1024, out_features=39, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nFormation du modèle étudiant terminée. Chargement du meilleur modèle pour l'évaluation.\")\n",
    "student_model.load_state_dict(torch.load('/workspace/models/best_student_mobilenetv3_model.pth'))\n",
    "student_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1] Time: 0.02s | CPU: 8.7% | RAM: 146.99GB | GPU: 304.67MB\n",
      "[Batch 2] Time: 0.01s | CPU: 7.0% | RAM: 147.01GB | GPU: 304.67MB\n",
      "[Batch 3] Time: 0.01s | CPU: 8.0% | RAM: 147.03GB | GPU: 304.67MB\n",
      "[Batch 4] Time: 0.01s | CPU: 5.5% | RAM: 147.04GB | GPU: 304.67MB\n",
      "[Batch 5] Time: 0.01s | CPU: 6.4% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 6] Time: 0.01s | CPU: 6.9% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 7] Time: 0.01s | CPU: 6.5% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 8] Time: 0.01s | CPU: 4.9% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 9] Time: 0.01s | CPU: 5.5% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 10] Time: 0.01s | CPU: 6.1% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 11] Time: 0.01s | CPU: 4.8% | RAM: 147.11GB | GPU: 304.67MB\n",
      "[Batch 12] Time: 0.01s | CPU: 5.6% | RAM: 147.12GB | GPU: 304.67MB\n",
      "[Batch 13] Time: 0.01s | CPU: 6.9% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 14] Time: 0.01s | CPU: 21.4% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 15] Time: 0.01s | CPU: 5.0% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 16] Time: 0.01s | CPU: 5.6% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 17] Time: 0.01s | CPU: 5.4% | RAM: 147.12GB | GPU: 304.67MB\n",
      "[Batch 18] Time: 0.01s | CPU: 9.9% | RAM: 147.12GB | GPU: 304.67MB\n",
      "[Batch 19] Time: 0.01s | CPU: 12.6% | RAM: 147.11GB | GPU: 304.67MB\n",
      "[Batch 20] Time: 0.01s | CPU: 5.6% | RAM: 147.11GB | GPU: 304.67MB\n",
      "[Batch 21] Time: 0.02s | CPU: 5.2% | RAM: 147.11GB | GPU: 304.67MB\n",
      "[Batch 22] Time: 0.01s | CPU: 5.1% | RAM: 147.11GB | GPU: 304.67MB\n",
      "[Batch 23] Time: 0.01s | CPU: 5.6% | RAM: 147.11GB | GPU: 304.67MB\n",
      "[Batch 24] Time: 0.01s | CPU: 6.0% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 25] Time: 0.01s | CPU: 6.2% | RAM: 147.11GB | GPU: 304.67MB\n",
      "[Batch 26] Time: 0.01s | CPU: 6.3% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 27] Time: 0.01s | CPU: 7.1% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 28] Time: 0.01s | CPU: 5.5% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 29] Time: 0.02s | CPU: 6.4% | RAM: 147.12GB | GPU: 304.67MB\n",
      "[Batch 30] Time: 0.01s | CPU: 6.1% | RAM: 147.12GB | GPU: 304.67MB\n",
      "[Batch 31] Time: 0.01s | CPU: 7.9% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 32] Time: 0.01s | CPU: 6.9% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 33] Time: 0.02s | CPU: 6.7% | RAM: 147.14GB | GPU: 304.67MB\n",
      "[Batch 34] Time: 0.01s | CPU: 5.5% | RAM: 147.14GB | GPU: 304.67MB\n",
      "[Batch 35] Time: 0.01s | CPU: 11.1% | RAM: 147.12GB | GPU: 304.67MB\n",
      "[Batch 36] Time: 0.01s | CPU: 43.8% | RAM: 147.12GB | GPU: 304.67MB\n",
      "[Batch 37] Time: 0.02s | CPU: 11.3% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 38] Time: 0.01s | CPU: 6.9% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 39] Time: 0.01s | CPU: 6.5% | RAM: 147.08GB | GPU: 304.67MB\n",
      "[Batch 40] Time: 0.01s | CPU: 7.1% | RAM: 147.08GB | GPU: 304.67MB\n",
      "[Batch 41] Time: 0.01s | CPU: 6.6% | RAM: 147.06GB | GPU: 304.67MB\n",
      "[Batch 42] Time: 0.01s | CPU: 6.3% | RAM: 147.05GB | GPU: 304.67MB\n",
      "[Batch 43] Time: 0.01s | CPU: 6.4% | RAM: 147.06GB | GPU: 304.67MB\n",
      "[Batch 44] Time: 0.01s | CPU: 8.1% | RAM: 147.06GB | GPU: 304.67MB\n",
      "[Batch 45] Time: 0.02s | CPU: 6.6% | RAM: 147.07GB | GPU: 304.67MB\n",
      "[Batch 46] Time: 0.01s | CPU: 7.5% | RAM: 147.06GB | GPU: 304.67MB\n",
      "[Batch 47] Time: 0.01s | CPU: 7.1% | RAM: 147.06GB | GPU: 304.67MB\n",
      "[Batch 48] Time: 0.01s | CPU: 7.0% | RAM: 147.06GB | GPU: 304.67MB\n",
      "[Batch 49] Time: 0.01s | CPU: 6.5% | RAM: 147.07GB | GPU: 304.67MB\n",
      "[Batch 50] Time: 0.01s | CPU: 8.1% | RAM: 147.07GB | GPU: 304.67MB\n",
      "[Batch 51] Time: 0.01s | CPU: 6.5% | RAM: 147.07GB | GPU: 304.67MB\n",
      "[Batch 52] Time: 0.01s | CPU: 6.5% | RAM: 147.07GB | GPU: 304.67MB\n",
      "[Batch 53] Time: 0.01s | CPU: 6.4% | RAM: 147.07GB | GPU: 304.67MB\n",
      "[Batch 54] Time: 0.01s | CPU: 6.7% | RAM: 147.08GB | GPU: 304.67MB\n",
      "[Batch 55] Time: 0.01s | CPU: 7.6% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 56] Time: 0.01s | CPU: 5.7% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 57] Time: 0.01s | CPU: 6.5% | RAM: 147.08GB | GPU: 304.67MB\n",
      "[Batch 58] Time: 0.01s | CPU: 33.3% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 59] Time: 0.01s | CPU: 16.4% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 60] Time: 0.01s | CPU: 6.9% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 61] Time: 0.01s | CPU: 6.4% | RAM: 147.06GB | GPU: 304.67MB\n",
      "[Batch 62] Time: 0.01s | CPU: 6.6% | RAM: 147.07GB | GPU: 304.67MB\n",
      "[Batch 63] Time: 0.01s | CPU: 5.2% | RAM: 147.08GB | GPU: 304.67MB\n",
      "[Batch 64] Time: 0.01s | CPU: 6.3% | RAM: 147.07GB | GPU: 304.67MB\n",
      "[Batch 65] Time: 0.02s | CPU: 6.3% | RAM: 147.07GB | GPU: 304.67MB\n",
      "[Batch 66] Time: 0.01s | CPU: 7.1% | RAM: 147.07GB | GPU: 304.67MB\n",
      "[Batch 67] Time: 0.01s | CPU: 7.4% | RAM: 147.06GB | GPU: 304.67MB\n",
      "[Batch 68] Time: 0.01s | CPU: 6.3% | RAM: 147.07GB | GPU: 304.67MB\n",
      "[Batch 69] Time: 0.01s | CPU: 6.1% | RAM: 147.08GB | GPU: 304.67MB\n",
      "[Batch 70] Time: 0.01s | CPU: 6.0% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 71] Time: 0.01s | CPU: 6.4% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 72] Time: 0.01s | CPU: 6.1% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 73] Time: 0.01s | CPU: 5.9% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 74] Time: 0.01s | CPU: 6.8% | RAM: 147.12GB | GPU: 304.67MB\n",
      "[Batch 75] Time: 0.01s | CPU: 7.1% | RAM: 147.11GB | GPU: 304.67MB\n",
      "[Batch 76] Time: 0.01s | CPU: 7.0% | RAM: 147.11GB | GPU: 304.67MB\n",
      "[Batch 77] Time: 0.01s | CPU: 6.4% | RAM: 147.11GB | GPU: 304.67MB\n",
      "[Batch 78] Time: 0.01s | CPU: 41.2% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 79] Time: 0.01s | CPU: 14.4% | RAM: 147.12GB | GPU: 304.67MB\n",
      "[Batch 80] Time: 0.01s | CPU: 8.4% | RAM: 147.12GB | GPU: 304.67MB\n",
      "[Batch 81] Time: 0.01s | CPU: 6.4% | RAM: 147.11GB | GPU: 304.67MB\n",
      "[Batch 82] Time: 0.01s | CPU: 7.3% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 83] Time: 0.01s | CPU: 6.8% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 84] Time: 0.01s | CPU: 6.5% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 85] Time: 0.01s | CPU: 6.8% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 86] Time: 0.01s | CPU: 7.5% | RAM: 147.15GB | GPU: 304.67MB\n",
      "[Batch 87] Time: 0.01s | CPU: 7.7% | RAM: 147.15GB | GPU: 304.67MB\n",
      "[Batch 88] Time: 0.01s | CPU: 8.0% | RAM: 147.14GB | GPU: 304.67MB\n",
      "[Batch 89] Time: 0.01s | CPU: 7.1% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 90] Time: 0.01s | CPU: 7.9% | RAM: 147.14GB | GPU: 304.67MB\n",
      "[Batch 91] Time: 0.01s | CPU: 7.3% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 92] Time: 0.01s | CPU: 6.3% | RAM: 147.14GB | GPU: 304.67MB\n",
      "[Batch 93] Time: 0.01s | CPU: 7.8% | RAM: 147.14GB | GPU: 304.67MB\n",
      "[Batch 94] Time: 0.01s | CPU: 10.4% | RAM: 147.15GB | GPU: 304.67MB\n",
      "[Batch 95] Time: 0.01s | CPU: 8.8% | RAM: 147.14GB | GPU: 304.67MB\n",
      "[Batch 96] Time: 0.01s | CPU: 9.3% | RAM: 147.15GB | GPU: 304.67MB\n",
      "[Batch 97] Time: 0.01s | CPU: 9.4% | RAM: 147.15GB | GPU: 304.67MB\n",
      "[Batch 98] Time: 0.01s | CPU: 8.3% | RAM: 147.16GB | GPU: 304.67MB\n",
      "[Batch 99] Time: 0.01s | CPU: 9.2% | RAM: 147.15GB | GPU: 304.67MB\n",
      "[Batch 100] Time: 0.01s | CPU: 6.9% | RAM: 147.15GB | GPU: 304.67MB\n",
      "[Batch 101] Time: 0.01s | CPU: 10.4% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 102] Time: 0.01s | CPU: 8.1% | RAM: 147.11GB | GPU: 304.67MB\n",
      "[Batch 103] Time: 0.01s | CPU: 8.9% | RAM: 147.12GB | GPU: 304.67MB\n",
      "[Batch 104] Time: 0.01s | CPU: 8.4% | RAM: 147.12GB | GPU: 304.67MB\n",
      "[Batch 105] Time: 0.01s | CPU: 7.6% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 106] Time: 0.01s | CPU: 9.6% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 107] Time: 0.01s | CPU: 8.8% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 108] Time: 0.01s | CPU: 8.2% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 109] Time: 0.02s | CPU: 7.0% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 110] Time: 0.01s | CPU: 7.6% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 111] Time: 0.01s | CPU: 7.8% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 112] Time: 0.01s | CPU: 9.7% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 113] Time: 0.01s | CPU: 7.8% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 114] Time: 0.01s | CPU: 8.6% | RAM: 147.08GB | GPU: 304.67MB\n",
      "[Batch 115] Time: 0.01s | CPU: 9.3% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 116] Time: 0.01s | CPU: 8.6% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 117] Time: 0.01s | CPU: 7.2% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 118] Time: 0.01s | CPU: 6.7% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 119] Time: 0.02s | CPU: 7.5% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 120] Time: 0.01s | CPU: 7.3% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 121] Time: 0.01s | CPU: 11.7% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 122] Time: 0.01s | CPU: 7.1% | RAM: 147.11GB | GPU: 304.67MB\n",
      "[Batch 123] Time: 0.01s | CPU: 7.1% | RAM: 147.11GB | GPU: 304.67MB\n",
      "[Batch 124] Time: 0.01s | CPU: 6.6% | RAM: 147.12GB | GPU: 304.67MB\n",
      "[Batch 125] Time: 0.01s | CPU: 7.4% | RAM: 147.12GB | GPU: 304.67MB\n",
      "[Batch 126] Time: 0.01s | CPU: 6.8% | RAM: 147.11GB | GPU: 304.67MB\n",
      "[Batch 127] Time: 0.01s | CPU: 6.8% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 128] Time: 0.01s | CPU: 7.6% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 129] Time: 0.01s | CPU: 11.5% | RAM: 147.14GB | GPU: 304.67MB\n",
      "[Batch 130] Time: 0.01s | CPU: 7.6% | RAM: 147.15GB | GPU: 304.67MB\n",
      "[Batch 131] Time: 0.01s | CPU: 8.5% | RAM: 147.16GB | GPU: 304.67MB\n",
      "[Batch 132] Time: 0.01s | CPU: 8.2% | RAM: 147.18GB | GPU: 304.67MB\n",
      "[Batch 133] Time: 0.01s | CPU: 7.2% | RAM: 147.17GB | GPU: 304.67MB\n",
      "[Batch 134] Time: 0.01s | CPU: 8.0% | RAM: 147.19GB | GPU: 304.67MB\n",
      "[Batch 135] Time: 0.01s | CPU: 8.5% | RAM: 147.20GB | GPU: 304.67MB\n",
      "[Batch 136] Time: 0.01s | CPU: 8.8% | RAM: 147.22GB | GPU: 304.67MB\n",
      "[Batch 137] Time: 0.01s | CPU: 7.1% | RAM: 147.20GB | GPU: 304.67MB\n",
      "[Batch 138] Time: 0.01s | CPU: 9.6% | RAM: 147.22GB | GPU: 304.67MB\n",
      "[Batch 139] Time: 0.01s | CPU: 7.3% | RAM: 147.21GB | GPU: 304.67MB\n",
      "[Batch 140] Time: 0.01s | CPU: 7.8% | RAM: 147.22GB | GPU: 304.67MB\n",
      "[Batch 141] Time: 0.01s | CPU: 8.2% | RAM: 147.17GB | GPU: 304.67MB\n",
      "[Batch 142] Time: 0.01s | CPU: 22.7% | RAM: 147.16GB | GPU: 304.67MB\n",
      "[Batch 143] Time: 0.01s | CPU: 6.3% | RAM: 147.15GB | GPU: 304.67MB\n",
      "[Batch 144] Time: 0.01s | CPU: 10.3% | RAM: 147.15GB | GPU: 304.67MB\n",
      "[Batch 145] Time: 0.01s | CPU: 7.5% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 146] Time: 0.01s | CPU: 7.6% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 147] Time: 0.01s | CPU: 8.8% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 148] Time: 0.01s | CPU: 7.5% | RAM: 147.11GB | GPU: 304.67MB\n",
      "[Batch 149] Time: 0.01s | CPU: 7.6% | RAM: 147.11GB | GPU: 304.67MB\n",
      "[Batch 150] Time: 0.01s | CPU: 8.6% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 151] Time: 0.01s | CPU: 7.4% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 152] Time: 0.01s | CPU: 8.5% | RAM: 147.14GB | GPU: 304.67MB\n",
      "[Batch 153] Time: 0.01s | CPU: 7.3% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 154] Time: 0.01s | CPU: 8.1% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 155] Time: 0.01s | CPU: 8.9% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 156] Time: 0.01s | CPU: 8.4% | RAM: 147.14GB | GPU: 304.67MB\n",
      "[Batch 157] Time: 0.01s | CPU: 7.4% | RAM: 147.15GB | GPU: 304.67MB\n",
      "[Batch 158] Time: 0.01s | CPU: 8.6% | RAM: 147.16GB | GPU: 304.67MB\n",
      "[Batch 159] Time: 0.01s | CPU: 6.6% | RAM: 147.17GB | GPU: 304.67MB\n",
      "[Batch 160] Time: 0.01s | CPU: 8.1% | RAM: 147.16GB | GPU: 304.67MB\n",
      "[Batch 161] Time: 0.01s | CPU: 7.6% | RAM: 147.17GB | GPU: 304.67MB\n",
      "[Batch 162] Time: 0.01s | CPU: 8.8% | RAM: 147.16GB | GPU: 304.67MB\n",
      "[Batch 163] Time: 0.01s | CPU: 8.8% | RAM: 147.16GB | GPU: 304.67MB\n",
      "[Batch 164] Time: 0.01s | CPU: 11.9% | RAM: 147.15GB | GPU: 304.67MB\n",
      "[Batch 165] Time: 0.01s | CPU: 10.1% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 166] Time: 0.01s | CPU: 7.1% | RAM: 147.15GB | GPU: 304.67MB\n",
      "[Batch 167] Time: 0.01s | CPU: 8.3% | RAM: 147.15GB | GPU: 304.67MB\n",
      "[Batch 168] Time: 0.01s | CPU: 8.0% | RAM: 147.15GB | GPU: 304.67MB\n",
      "[Batch 169] Time: 0.01s | CPU: 7.5% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 170] Time: 0.01s | CPU: 7.8% | RAM: 147.14GB | GPU: 304.67MB\n",
      "[Batch 171] Time: 0.01s | CPU: 8.7% | RAM: 147.13GB | GPU: 304.67MB\n",
      "[Batch 172] Time: 0.01s | CPU: 8.9% | RAM: 147.12GB | GPU: 304.67MB\n",
      "[Batch 173] Time: 0.01s | CPU: 7.0% | RAM: 147.11GB | GPU: 304.67MB\n",
      "[Batch 174] Time: 0.01s | CPU: 8.4% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 175] Time: 0.01s | CPU: 6.8% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 176] Time: 0.01s | CPU: 7.6% | RAM: 147.10GB | GPU: 304.67MB\n",
      "[Batch 177] Time: 0.01s | CPU: 6.9% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 178] Time: 0.01s | CPU: 8.0% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 179] Time: 0.01s | CPU: 7.8% | RAM: 147.08GB | GPU: 304.67MB\n",
      "[Batch 180] Time: 0.01s | CPU: 8.3% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 181] Time: 0.01s | CPU: 7.1% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 182] Time: 0.01s | CPU: 7.6% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 183] Time: 0.01s | CPU: 7.5% | RAM: 147.08GB | GPU: 304.67MB\n",
      "[Batch 184] Time: 0.01s | CPU: 7.7% | RAM: 147.08GB | GPU: 304.67MB\n",
      "[Batch 185] Time: 0.01s | CPU: 13.3% | RAM: 147.08GB | GPU: 304.67MB\n",
      "[Batch 186] Time: 0.01s | CPU: 7.9% | RAM: 147.09GB | GPU: 304.67MB\n",
      "[Batch 187] Time: 0.01s | CPU: 8.5% | RAM: 147.08GB | GPU: 304.67MB\n",
      "[Batch 188] Time: 0.01s | CPU: 8.7% | RAM: 147.08GB | GPU: 304.67MB\n",
      "[Batch 189] Time: 0.01s | CPU: 6.6% | RAM: 147.06GB | GPU: 304.67MB\n",
      "[Batch 190] Time: 0.01s | CPU: 6.3% | RAM: 147.05GB | GPU: 304.67MB\n",
      "[Batch 191] Time: 0.01s | CPU: 5.9% | RAM: 147.04GB | GPU: 304.67MB\n",
      "[Batch 192] Time: 0.01s | CPU: 6.0% | RAM: 147.05GB | GPU: 304.67MB\n",
      "[Batch 193] Time: 0.03s | CPU: 6.4% | RAM: 147.05GB | GPU: 289.29MB\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "        batch_start = time.time()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = student_model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # --- Profiling ---\n",
    "        cpu_usage = psutil.cpu_percent(interval=None)\n",
    "        ram = psutil.virtual_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "        else:\n",
    "            gpu_mem = 0.0\n",
    "        print(f\"[Batch {i+1}] Time: {time.time()-batch_start:.2f}s | CPU: {cpu_usage:.1f}% | RAM: {ram.used/1024**3:.2f}GB | GPU: {gpu_mem:.2f}MB\")\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temps Test Total: 8.74 sec\n",
      "Throughput: 703.73 images/sec\n"
     ]
    }
   ],
   "source": [
    "total_time = end_time - start_time\n",
    "print(f\"\\nTemps Test Total: {total_time:.2f} sec\")\n",
    "print(f\"Throughput: {len(test_dataset) / total_time:.2f} images/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Set Evaluation (Student) ===\n",
      "                                               precision    recall  f1-score   support\n",
      "\n",
      "                           Apple___Apple_scab       1.00      0.99      0.99       100\n",
      "                            Apple___Black_rot       1.00      1.00      1.00       100\n",
      "                     Apple___Cedar_apple_rust       1.00      1.00      1.00       100\n",
      "                              Apple___healthy       0.99      0.99      0.99       164\n",
      "                    Background_without_leaves       0.97      1.00      0.99       114\n",
      "                          Blueberry___healthy       1.00      1.00      1.00       150\n",
      "                      Cherry___Powdery_mildew       1.00      1.00      1.00       105\n",
      "                             Cherry___healthy       1.00      0.99      0.99       100\n",
      "   Corn___Cercospora_leaf_spot Gray_leaf_spot       0.98      0.99      0.99       100\n",
      "                           Corn___Common_rust       0.99      1.00      1.00       119\n",
      "                  Corn___Northern_Leaf_Blight       0.98      0.97      0.97       100\n",
      "                               Corn___healthy       1.00      0.99      1.00       116\n",
      "                            Grape___Black_rot       1.00      1.00      1.00       118\n",
      "                 Grape___Esca_(Black_Measles)       1.00      0.99      1.00       138\n",
      "   Grape___Leaf_blight_(Isariopsis_Leaf_Spot)       0.99      1.00      1.00       108\n",
      "                              Grape___healthy       1.00      1.00      1.00       100\n",
      "     Orange___Haunglongbing_(Citrus_greening)       1.00      1.00      1.00       551\n",
      "                       Peach___Bacterial_spot       1.00      1.00      1.00       230\n",
      "                              Peach___healthy       1.00      1.00      1.00       100\n",
      "                Pepper,_bell___Bacterial_spot       1.00      1.00      1.00       100\n",
      "                       Pepper,_bell___healthy       1.00      1.00      1.00       148\n",
      "                        Potato___Early_blight       1.00      1.00      1.00       100\n",
      "                         Potato___Late_blight       1.00      1.00      1.00       100\n",
      "                             Potato___healthy       1.00      1.00      1.00       100\n",
      "                          Raspberry___healthy       1.00      1.00      1.00       100\n",
      "                            Soybean___healthy       1.00      1.00      1.00       509\n",
      "                      Squash___Powdery_mildew       1.00      1.00      1.00       184\n",
      "                     Strawberry___Leaf_scorch       1.00      0.99      1.00       111\n",
      "                         Strawberry___healthy       1.00      1.00      1.00       100\n",
      "                      Tomato___Bacterial_spot       1.00      1.00      1.00       213\n",
      "                        Tomato___Early_blight       0.99      1.00      1.00       100\n",
      "                         Tomato___Late_blight       0.99      1.00      1.00       191\n",
      "                           Tomato___Leaf_Mold       1.00      0.99      0.99       100\n",
      "                  Tomato___Septoria_leaf_spot       1.00      0.99      1.00       177\n",
      "Tomato___Spider_mites Two-spotted_spider_mite       0.99      0.98      0.99       168\n",
      "                         Tomato___Target_Spot       0.98      0.99      0.99       140\n",
      "       Tomato___Tomato_Yellow_Leaf_Curl_Virus       1.00      1.00      1.00       536\n",
      "                 Tomato___Tomato_mosaic_virus       1.00      1.00      1.00       100\n",
      "                             Tomato___healthy       1.00      0.99      1.00       159\n",
      "\n",
      "                                     accuracy                           1.00      6149\n",
      "                                    macro avg       1.00      1.00      1.00      6149\n",
      "                                 weighted avg       1.00      1.00      1.00      6149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rapport complet\n",
    "print(\"=== Test Set Evaluation (Student) ===\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

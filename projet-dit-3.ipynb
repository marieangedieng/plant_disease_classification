{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.9.9-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.1.2)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (77.0.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.3-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.74.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.11.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (107 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Collecting array_record>=0.5.0 (from tensorflow_datasets)\n",
      "  Downloading array_record-0.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (899 bytes)\n",
      "Collecting dm-tree (from tensorflow_datasets)\n",
      "  Downloading dm_tree-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting etils>=1.9.1 (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets)\n",
      "  Downloading etils-1.13.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting immutabledict (from tensorflow_datasets)\n",
      "  Downloading immutabledict-4.2.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (7.0.0)\n",
      "Collecting pyarrow (from tensorflow_datasets)\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting simple_parsing (from tensorflow_datasets)\n",
      "  Downloading simple_parsing-0.1.7-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Downloading tensorflow_metadata-1.17.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting toml (from tensorflow_datasets)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tqdm (from tensorflow_datasets)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting einops (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2024.10.0)\n",
      "Collecting importlib_resources (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: zipp in /usr/lib/python3/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.0.0)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.17.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow_datasets) (25.3.0)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple_parsing->tensorflow_datasets)\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow_datasets)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m573.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m154.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_datasets-4.9.9-py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m143.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading array_record-0.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m274.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading etils-1.13.0-py3-none-any.whl (170 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading fonttools-4.59.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m180.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.74.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m337.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m277.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading keras-3.11.2-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m342.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m175.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m263.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m272.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m324.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m321.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading wrapt-1.17.3-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (82 kB)\n",
      "Downloading dm_tree-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "Downloading immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
      "Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m275.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
      "Downloading tensorflow_metadata-1.17.2-py3-none-any.whl (31 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.17.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (402 kB)\n",
      "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21583 sha256=db9502a0167748b4107a7c94bb6983e1e3cd5ec22ac9c625463db1d22b737de2\n",
      "  Stored in directory: /root/.cache/pip/wheels/90/74/b1/9b54c896b8d9409e9268329d4d45ede8a8040abe91c8879932\n",
      "Successfully built promise\n",
      "Installing collected packages: pytz, namex, libclang, flatbuffers, wrapt, werkzeug, tzdata, tqdm, toml, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, scipy, pyarrow, protobuf, promise, optree, opt-einsum, opencv-python, ml-dtypes, mdurl, markdown, kiwisolver, joblib, importlib_resources, immutabledict, h5py, grpcio, google-pasta, gast, fonttools, etils, einops, docstring-parser, cycler, contourpy, astunparse, absl-py, tensorboard, simple_parsing, scikit-learn, pandas, matplotlib, markdown-it-py, googleapis-common-protos, dm-tree, tensorflow-metadata, rich, keras, array_record, tensorflow, tensorflow_datasets\n",
      "Successfully installed absl-py-2.3.1 array_record-0.7.2 astunparse-1.6.3 contourpy-1.3.3 cycler-0.12.1 dm-tree-0.1.9 docstring-parser-0.17.0 einops-0.8.1 etils-1.13.0 flatbuffers-25.2.10 fonttools-4.59.0 gast-0.6.0 google-pasta-0.2.0 googleapis-common-protos-1.70.0 grpcio-1.74.0 h5py-3.14.0 immutabledict-4.2.1 importlib_resources-6.5.2 joblib-1.5.1 keras-3.11.2 kiwisolver-1.4.9 libclang-18.1.1 markdown-3.8.2 markdown-it-py-4.0.0 matplotlib-3.10.5 mdurl-0.1.2 ml-dtypes-0.5.3 namex-0.1.0 opencv-python-4.12.0.88 opt-einsum-3.4.0 optree-0.17.0 pandas-2.3.1 promise-2.3 protobuf-5.29.5 pyarrow-21.0.0 pytz-2025.2 rich-14.1.0 scikit-learn-1.7.1 scipy-1.16.1 simple_parsing-0.1.7 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 tensorflow-metadata-1.17.2 tensorflow_datasets-4.9.9 termcolor-3.1.0 threadpoolctl-3.6.0 toml-0.10.2 tqdm-4.67.1 tzdata-2025.2 werkzeug-3.1.3 wrapt-1.17.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U opencv-python tensorflow scikit-learn pandas matplotlib tensorflow_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTATION DES LIBRAIRIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 08:08:36.626565: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755072516.640318     409 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755072516.644866     409 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755072516.655804     409 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755072516.655817     409 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755072516.655819     409 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755072516.655820     409 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-13 08:08:36.659456: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets.public_api as tfds\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def telecharger_dezip(url, chemin_sauv=\"plant_village_dataset.zip\", extract_path=\".\"):\n",
    "    print(\" Début du téléchargement\")\n",
    "    try:\n",
    "        response=requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        #Taille totale du fichier pour la barre de progression\n",
    "        total_size=int(response.headers.get('content-length',0))\n",
    "        block_size=1064\n",
    "        bar_progression = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "\n",
    "        #Téléchargement\n",
    "        with open(chemin_sauv, 'wb') as file:\n",
    "            for data in response.iter_content(block_size):\n",
    "                bar_progression.update(len(data))\n",
    "                file.write(data)\n",
    "        bar_progression.close()\n",
    "\n",
    "        if total_size != 0 and bar_progression.n != total_size:\n",
    "            print(\"ERREUR, quelque chose s'est mal passé pendant le téléchargement.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Téléchargement terminé. Fichier sauvegardé sous : {chemin_sauv}\")\n",
    "\n",
    "        # Créer le dossier d'extraction s'il n'existe pas\n",
    "        if not os.path.exists(extract_path):\n",
    "            os.makedirs(extract_path)\n",
    "\n",
    "        # Décompresser le fichier ZIP\n",
    "        print(f\"Décompression du fichier dans le dossier : {extract_path}\")\n",
    "        with zipfile.ZipFile(chemin_sauv, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "\n",
    "        print(\"Décompression terminée.\")\n",
    "\n",
    "        # Optionnel : Supprimer le fichier .zip après extraction pour économiser de l'espace\n",
    "        print(f\"Suppression du fichier {chemin_sauv}...\")\n",
    "        os.remove(chemin_sauv)\n",
    "        print(\"Opération terminée avec succès !\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Une erreur de réseau est survenue: {e}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"Erreur: Le fichier téléchargé n'est pas un fichier ZIP valide.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur inattendue est survenue: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://data.mendeley.com/datasets/tywbtsjrjv/1/files/b4e3a32f-c0bd-4060-81e9-6144231f2520/file_downloaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_folder = \"plant_village_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Début du téléchargement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 949M/949M [00:35<00:00, 26.7MiB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement terminé. Fichier sauvegardé sous : PlantVillage.zip\n",
      "Décompression du fichier dans le dossier : plant_village_dataset\n",
      "Décompression terminée.\n",
      "Suppression du fichier PlantVillage.zip...\n",
      "Opération terminée avec succès !\n"
     ]
    }
   ],
   "source": [
    "telecharger_dezip(URL, \"PlantVillage.zip\", extract_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/workspace/plant_village_dataset/Plant_leave_diseases_dataset_with_augmentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (50, 50)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61486 images belonging to 39 classes.\n"
     ]
    }
   ],
   "source": [
    "data=data_gen.flow_from_directory(\n",
    "    path,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personnalisé CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import psutil\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total des images trouvées : 61486\n"
     ]
    }
   ],
   "source": [
    "# --------- 1. Préparer les données ---------\n",
    "filepaths = []\n",
    "labels = []\n",
    "folds = os.listdir(path)\n",
    "for fold in folds:\n",
    "    f_path = os.path.join(path, fold)\n",
    "    if not os.path.isdir(f_path):\n",
    "        continue\n",
    "    for file in os.listdir(f_path):\n",
    "        filepaths.append(os.path.join(f_path, file))\n",
    "        labels.append(fold)\n",
    "\n",
    "df = pd.DataFrame({'filepaths': filepaths, 'labels': labels})\n",
    "print(f\"Total des images trouvées : {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 80/20 avec stratification\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=df['labels']\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df['labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping des classes en indices\n",
    "class_names = sorted(df['labels'].unique())\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- 2. Dataset personnalisé ---------\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, class_to_idx, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.loc[idx, 'filepaths']\n",
    "        label_name = self.df.loc[idx, 'labels']\n",
    "        label = self.class_to_idx[label_name]\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- 3. Data augmentation et loaders ---------\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]) # EfficientNet normalization\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomImageDataset(train_df, class_to_idx, transform=train_transforms)\n",
    "val_dataset = CustomImageDataset(val_df, class_to_idx, transform=val_transforms)\n",
    "test_dataset = CustomImageDataset(test_df, class_to_idx, transform=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DeepCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conav2d(3, 32, kernel_size=3, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=0)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=0)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=0)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Calcul automatique de la taille après convolution/pooling\n",
    "        self.flatten_dim = self._get_flatten_dim()\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.flatten_dim, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def _get_flatten_dim(self):\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, 3, 50, 50)  # batch=1, channels=3, H=50, W=50\n",
    "            x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
    "            x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
    "            return x.view(1, -1).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deepcnn = DeepCNN(num_classes=len(class_names)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de perte (équivalent à categorical_crossentropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimiseur (équivalent à optimizer='adam')\n",
    "optimizer = optim.Adam(model_deepcnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"/workspace/models/best_model_deepcnn.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0.0\n",
    "epochs_no_improve = 0\n",
    "patience = 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Train loss: 2.4761, acc: 0.3211 | Val loss: 1.4572, acc: 0.5517\n",
      ">> Nouveau meilleur modèle sauvegardé\n",
      "Epoch [2/20] - Train loss: 1.7744, acc: 0.4891 | Val loss: 0.9525, acc: 0.6987\n",
      ">> Nouveau meilleur modèle sauvegardé\n",
      "Epoch [3/20] - Train loss: 1.4546, acc: 0.5730 | Val loss: 0.7672, acc: 0.7551\n",
      ">> Nouveau meilleur modèle sauvegardé\n",
      "Epoch [4/20] - Train loss: 1.2781, acc: 0.6216 | Val loss: 0.5962, acc: 0.8037\n",
      ">> Nouveau meilleur modèle sauvegardé\n",
      "Epoch [5/20] - Train loss: 1.1465, acc: 0.6588 | Val loss: 0.5249, acc: 0.8345\n",
      ">> Nouveau meilleur modèle sauvegardé\n",
      "Epoch [6/20] - Train loss: 1.0538, acc: 0.6847 | Val loss: 0.4221, acc: 0.8657\n",
      ">> Nouveau meilleur modèle sauvegardé\n",
      "Epoch [7/20] - Train loss: 0.9795, acc: 0.7079 | Val loss: 0.4121, acc: 0.8666\n",
      ">> Nouveau meilleur modèle sauvegardé\n",
      "Epoch [8/20] - Train loss: 0.9430, acc: 0.7177 | Val loss: 0.3651, acc: 0.8838\n",
      ">> Nouveau meilleur modèle sauvegardé\n",
      "Epoch [9/20] - Train loss: 0.9043, acc: 0.7293 | Val loss: 0.3074, acc: 0.8968\n",
      ">> Nouveau meilleur modèle sauvegardé\n",
      "Epoch [10/20] - Train loss: 0.8741, acc: 0.7375 | Val loss: 0.3098, acc: 0.8993\n",
      ">> Nouveau meilleur modèle sauvegardé\n",
      "Epoch [11/20] - Train loss: 0.8475, acc: 0.7480 | Val loss: 0.2984, acc: 0.9024\n",
      ">> Nouveau meilleur modèle sauvegardé\n",
      "Epoch [12/20] - Train loss: 0.8262, acc: 0.7536 | Val loss: 0.2872, acc: 0.9060\n",
      ">> Nouveau meilleur modèle sauvegardé\n",
      "Epoch [13/20] - Train loss: 0.8188, acc: 0.7568 | Val loss: 0.3071, acc: 0.9024\n",
      "Epoch [14/20] - Train loss: 0.7875, acc: 0.7649 | Val loss: 0.2507, acc: 0.9213\n",
      ">> Nouveau meilleur modèle sauvegardé\n",
      "Epoch [15/20] - Train loss: 0.7857, acc: 0.7679 | Val loss: 0.2681, acc: 0.9173\n",
      "Epoch [16/20] - Train loss: 0.7510, acc: 0.7744 | Val loss: 0.2619, acc: 0.9204\n",
      "Epoch [17/20] - Train loss: 0.7549, acc: 0.7744 | Val loss: 0.2463, acc: 0.9225\n",
      ">> Nouveau meilleur modèle sauvegardé\n",
      "Epoch [18/20] - Train loss: 0.7502, acc: 0.7766 | Val loss: 0.2520, acc: 0.9199\n",
      "Epoch [19/20] - Train loss: 0.7242, acc: 0.7825 | Val loss: 0.2181, acc: 0.9361\n",
      ">> Nouveau meilleur modèle sauvegardé\n",
      "Epoch [20/20] - Train loss: 0.7265, acc: 0.7840 | Val loss: 0.2230, acc: 0.9308\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    # --- TRAIN ---\n",
    "    model_deepcnn.train()\n",
    "    train_loss, train_correct = 0.0, 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_deepcnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = train_correct / len(train_loader.dataset)\n",
    "\n",
    "    # --- VALIDATION ---\n",
    "    model_deepcnn.eval()\n",
    "    val_loss, val_correct = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_deepcnn(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - \"\n",
    "          f\"Train loss: {train_loss:.4f}, acc: {train_acc:.4f} | \"\n",
    "          f\"Val loss: {val_loss:.4f}, acc: {val_acc:.4f}\")\n",
    "\n",
    "    # --- Early stopping & checkpoint ---\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model_deepcnn.state_dict(), best_model_path)\n",
    "        print(\">> Nouveau meilleur modèle sauvegardé\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\">> Early stopping déclenché\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le meilleur modèle sauvegardé\n",
    "model_deepcnn.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "model_deepcnn.to(device)\n",
    "model_deepcnn.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1] Temps: 0.00s | CPU: 6.8% | RAM: 193.63GB | GPU: 38.59MB\n",
      "[Batch 2] Temps: 0.00s | CPU: 3.8% | RAM: 193.60GB | GPU: 38.59MB\n",
      "[Batch 3] Temps: 0.00s | CPU: 4.2% | RAM: 193.60GB | GPU: 38.59MB\n",
      "[Batch 4] Temps: 0.00s | CPU: 2.5% | RAM: 193.55GB | GPU: 38.59MB\n",
      "[Batch 5] Temps: 0.00s | CPU: 15.0% | RAM: 192.79GB | GPU: 38.59MB\n",
      "[Batch 6] Temps: 0.00s | CPU: 52.1% | RAM: 192.75GB | GPU: 38.59MB\n",
      "[Batch 7] Temps: 0.00s | CPU: 4.8% | RAM: 192.22GB | GPU: 38.59MB\n",
      "[Batch 8] Temps: 0.00s | CPU: 5.4% | RAM: 192.19GB | GPU: 38.59MB\n",
      "[Batch 9] Temps: 0.00s | CPU: 4.5% | RAM: 191.40GB | GPU: 38.59MB\n",
      "[Batch 10] Temps: 0.00s | CPU: 6.2% | RAM: 191.35GB | GPU: 38.59MB\n",
      "[Batch 11] Temps: 0.00s | CPU: 5.7% | RAM: 190.96GB | GPU: 38.59MB\n",
      "[Batch 12] Temps: 0.00s | CPU: 7.4% | RAM: 190.94GB | GPU: 38.59MB\n",
      "[Batch 13] Temps: 0.00s | CPU: 5.6% | RAM: 190.40GB | GPU: 38.59MB\n",
      "[Batch 14] Temps: 0.00s | CPU: 8.3% | RAM: 190.37GB | GPU: 38.59MB\n",
      "[Batch 15] Temps: 0.00s | CPU: 8.6% | RAM: 190.02GB | GPU: 38.59MB\n",
      "[Batch 16] Temps: 0.00s | CPU: 14.8% | RAM: 190.01GB | GPU: 38.59MB\n",
      "[Batch 17] Temps: 0.00s | CPU: 3.1% | RAM: 190.01GB | GPU: 38.59MB\n",
      "[Batch 18] Temps: 0.00s | CPU: 3.8% | RAM: 190.02GB | GPU: 38.59MB\n",
      "[Batch 19] Temps: 0.00s | CPU: 4.3% | RAM: 190.00GB | GPU: 38.59MB\n",
      "[Batch 20] Temps: 0.00s | CPU: 3.5% | RAM: 190.01GB | GPU: 38.59MB\n",
      "[Batch 21] Temps: 0.00s | CPU: 4.5% | RAM: 190.02GB | GPU: 38.59MB\n",
      "[Batch 22] Temps: 0.00s | CPU: 7.5% | RAM: 190.02GB | GPU: 38.59MB\n",
      "[Batch 23] Temps: 0.00s | CPU: 18.3% | RAM: 190.14GB | GPU: 38.59MB\n",
      "[Batch 24] Temps: 0.00s | CPU: 52.8% | RAM: 190.08GB | GPU: 38.59MB\n",
      "[Batch 25] Temps: 0.00s | CPU: 11.9% | RAM: 189.36GB | GPU: 38.59MB\n",
      "[Batch 26] Temps: 0.00s | CPU: 54.2% | RAM: 189.31GB | GPU: 38.59MB\n",
      "[Batch 27] Temps: 0.00s | CPU: 4.8% | RAM: 189.38GB | GPU: 38.59MB\n",
      "[Batch 28] Temps: 0.00s | CPU: 5.8% | RAM: 189.37GB | GPU: 38.59MB\n",
      "[Batch 29] Temps: 0.00s | CPU: 2.2% | RAM: 189.37GB | GPU: 38.59MB\n",
      "[Batch 30] Temps: 0.00s | CPU: 7.0% | RAM: 189.38GB | GPU: 38.59MB\n",
      "[Batch 31] Temps: 0.00s | CPU: 12.8% | RAM: 190.03GB | GPU: 38.59MB\n",
      "[Batch 32] Temps: 0.00s | CPU: 5.7% | RAM: 189.93GB | GPU: 38.59MB\n",
      "[Batch 33] Temps: 0.00s | CPU: 3.6% | RAM: 189.88GB | GPU: 38.59MB\n",
      "[Batch 34] Temps: 0.00s | CPU: 6.5% | RAM: 189.80GB | GPU: 38.59MB\n",
      "[Batch 35] Temps: 0.00s | CPU: 9.7% | RAM: 189.59GB | GPU: 38.59MB\n",
      "[Batch 36] Temps: 0.00s | CPU: 3.8% | RAM: 189.57GB | GPU: 38.59MB\n",
      "[Batch 37] Temps: 0.00s | CPU: 4.0% | RAM: 189.57GB | GPU: 38.59MB\n",
      "[Batch 38] Temps: 0.00s | CPU: 8.0% | RAM: 189.55GB | GPU: 38.59MB\n",
      "[Batch 39] Temps: 0.00s | CPU: 4.9% | RAM: 189.51GB | GPU: 38.59MB\n",
      "[Batch 40] Temps: 0.00s | CPU: 6.9% | RAM: 189.51GB | GPU: 38.59MB\n",
      "[Batch 41] Temps: 0.00s | CPU: 4.5% | RAM: 189.52GB | GPU: 38.59MB\n",
      "[Batch 42] Temps: 0.00s | CPU: 4.5% | RAM: 189.53GB | GPU: 38.59MB\n",
      "[Batch 43] Temps: 0.00s | CPU: 5.2% | RAM: 189.56GB | GPU: 38.59MB\n",
      "[Batch 44] Temps: 0.00s | CPU: 9.1% | RAM: 189.56GB | GPU: 38.59MB\n",
      "[Batch 45] Temps: 0.00s | CPU: 6.2% | RAM: 189.55GB | GPU: 38.59MB\n",
      "[Batch 46] Temps: 0.00s | CPU: 6.7% | RAM: 189.53GB | GPU: 38.59MB\n",
      "[Batch 47] Temps: 0.00s | CPU: 5.8% | RAM: 189.58GB | GPU: 38.59MB\n",
      "[Batch 48] Temps: 0.00s | CPU: 6.7% | RAM: 189.55GB | GPU: 38.59MB\n",
      "[Batch 49] Temps: 0.00s | CPU: 4.0% | RAM: 189.56GB | GPU: 38.59MB\n",
      "[Batch 50] Temps: 0.00s | CPU: 10.7% | RAM: 189.58GB | GPU: 38.59MB\n",
      "[Batch 51] Temps: 0.00s | CPU: 7.6% | RAM: 189.59GB | GPU: 38.59MB\n",
      "[Batch 52] Temps: 0.00s | CPU: 8.1% | RAM: 189.61GB | GPU: 38.59MB\n",
      "[Batch 53] Temps: 0.00s | CPU: 9.1% | RAM: 189.63GB | GPU: 38.59MB\n",
      "[Batch 54] Temps: 0.00s | CPU: 5.9% | RAM: 189.64GB | GPU: 38.59MB\n",
      "[Batch 55] Temps: 0.00s | CPU: 14.9% | RAM: 189.65GB | GPU: 38.59MB\n",
      "[Batch 56] Temps: 0.00s | CPU: 16.7% | RAM: 189.68GB | GPU: 38.59MB\n",
      "[Batch 57] Temps: 0.00s | CPU: 8.7% | RAM: 189.67GB | GPU: 38.59MB\n",
      "[Batch 58] Temps: 0.00s | CPU: 8.2% | RAM: 189.70GB | GPU: 38.59MB\n",
      "[Batch 59] Temps: 0.00s | CPU: 10.2% | RAM: 189.70GB | GPU: 38.59MB\n",
      "[Batch 60] Temps: 0.00s | CPU: 7.7% | RAM: 189.73GB | GPU: 38.59MB\n",
      "[Batch 61] Temps: 0.00s | CPU: 8.4% | RAM: 189.74GB | GPU: 38.59MB\n",
      "[Batch 62] Temps: 0.00s | CPU: 8.0% | RAM: 189.79GB | GPU: 38.59MB\n",
      "[Batch 63] Temps: 0.00s | CPU: 9.5% | RAM: 189.80GB | GPU: 38.59MB\n",
      "[Batch 64] Temps: 0.00s | CPU: 12.2% | RAM: 189.81GB | GPU: 38.59MB\n",
      "[Batch 65] Temps: 0.00s | CPU: 8.8% | RAM: 189.81GB | GPU: 38.59MB\n",
      "[Batch 66] Temps: 0.00s | CPU: 15.2% | RAM: 189.83GB | GPU: 38.59MB\n",
      "[Batch 67] Temps: 0.00s | CPU: 9.2% | RAM: 189.89GB | GPU: 38.59MB\n",
      "[Batch 68] Temps: 0.00s | CPU: 8.3% | RAM: 189.90GB | GPU: 38.59MB\n",
      "[Batch 69] Temps: 0.00s | CPU: 9.9% | RAM: 189.90GB | GPU: 38.59MB\n",
      "[Batch 70] Temps: 0.00s | CPU: 10.6% | RAM: 189.90GB | GPU: 38.59MB\n",
      "[Batch 71] Temps: 0.00s | CPU: 8.9% | RAM: 189.93GB | GPU: 38.59MB\n",
      "[Batch 72] Temps: 0.00s | CPU: 11.9% | RAM: 189.93GB | GPU: 38.59MB\n",
      "[Batch 73] Temps: 0.00s | CPU: 8.5% | RAM: 189.97GB | GPU: 38.59MB\n",
      "[Batch 74] Temps: 0.00s | CPU: 9.4% | RAM: 189.97GB | GPU: 38.59MB\n",
      "[Batch 75] Temps: 0.00s | CPU: 11.2% | RAM: 189.97GB | GPU: 38.59MB\n",
      "[Batch 76] Temps: 0.00s | CPU: 15.2% | RAM: 189.95GB | GPU: 38.59MB\n",
      "[Batch 77] Temps: 0.00s | CPU: 10.5% | RAM: 189.68GB | GPU: 38.59MB\n",
      "[Batch 78] Temps: 0.00s | CPU: 12.2% | RAM: 189.68GB | GPU: 38.59MB\n",
      "[Batch 79] Temps: 0.00s | CPU: 41.7% | RAM: 189.77GB | GPU: 38.59MB\n",
      "[Batch 80] Temps: 0.00s | CPU: 59.0% | RAM: 189.77GB | GPU: 38.59MB\n",
      "[Batch 81] Temps: 0.00s | CPU: 8.4% | RAM: 189.81GB | GPU: 38.59MB\n",
      "[Batch 82] Temps: 0.00s | CPU: 13.0% | RAM: 189.82GB | GPU: 38.59MB\n",
      "[Batch 83] Temps: 0.00s | CPU: 18.2% | RAM: 189.82GB | GPU: 38.59MB\n",
      "[Batch 84] Temps: 0.00s | CPU: 7.7% | RAM: 189.82GB | GPU: 38.59MB\n",
      "[Batch 85] Temps: 0.00s | CPU: 7.4% | RAM: 189.70GB | GPU: 38.59MB\n",
      "[Batch 86] Temps: 0.00s | CPU: 4.4% | RAM: 189.70GB | GPU: 38.59MB\n",
      "[Batch 87] Temps: 0.00s | CPU: 8.7% | RAM: 189.70GB | GPU: 38.59MB\n",
      "[Batch 88] Temps: 0.00s | CPU: 4.5% | RAM: 189.70GB | GPU: 38.59MB\n",
      "[Batch 89] Temps: 0.00s | CPU: 8.3% | RAM: 189.71GB | GPU: 38.59MB\n",
      "[Batch 90] Temps: 0.00s | CPU: 8.0% | RAM: 189.72GB | GPU: 38.59MB\n",
      "[Batch 91] Temps: 0.00s | CPU: 6.8% | RAM: 189.72GB | GPU: 38.59MB\n",
      "[Batch 92] Temps: 0.00s | CPU: 10.9% | RAM: 189.72GB | GPU: 38.59MB\n",
      "[Batch 93] Temps: 0.00s | CPU: 11.2% | RAM: 189.70GB | GPU: 38.59MB\n",
      "[Batch 94] Temps: 0.00s | CPU: 11.1% | RAM: 189.68GB | GPU: 38.59MB\n",
      "[Batch 95] Temps: 0.00s | CPU: 11.4% | RAM: 189.70GB | GPU: 38.59MB\n",
      "[Batch 96] Temps: 0.00s | CPU: 11.1% | RAM: 189.70GB | GPU: 38.59MB\n",
      "[Batch 97] Temps: 0.00s | CPU: 7.7% | RAM: 189.72GB | GPU: 38.59MB\n",
      "[Batch 98] Temps: 0.00s | CPU: 11.4% | RAM: 189.72GB | GPU: 38.59MB\n",
      "[Batch 99] Temps: 0.00s | CPU: 8.7% | RAM: 189.72GB | GPU: 38.59MB\n",
      "[Batch 100] Temps: 0.00s | CPU: 7.0% | RAM: 189.72GB | GPU: 38.59MB\n",
      "[Batch 101] Temps: 0.00s | CPU: 7.8% | RAM: 189.72GB | GPU: 38.59MB\n",
      "[Batch 102] Temps: 0.00s | CPU: 11.8% | RAM: 189.73GB | GPU: 38.59MB\n",
      "[Batch 103] Temps: 0.00s | CPU: 11.4% | RAM: 189.74GB | GPU: 38.59MB\n",
      "[Batch 104] Temps: 0.00s | CPU: 9.1% | RAM: 189.74GB | GPU: 38.59MB\n",
      "[Batch 105] Temps: 0.00s | CPU: 9.4% | RAM: 189.80GB | GPU: 38.59MB\n",
      "[Batch 106] Temps: 0.00s | CPU: 11.4% | RAM: 189.80GB | GPU: 38.59MB\n",
      "[Batch 107] Temps: 0.00s | CPU: 17.4% | RAM: 189.80GB | GPU: 38.59MB\n",
      "[Batch 108] Temps: 0.00s | CPU: 15.6% | RAM: 189.81GB | GPU: 38.59MB\n",
      "[Batch 109] Temps: 0.00s | CPU: 15.8% | RAM: 189.91GB | GPU: 38.59MB\n",
      "[Batch 110] Temps: 0.00s | CPU: 12.0% | RAM: 189.91GB | GPU: 38.59MB\n",
      "[Batch 111] Temps: 0.00s | CPU: 10.5% | RAM: 189.91GB | GPU: 38.59MB\n",
      "[Batch 112] Temps: 0.00s | CPU: 10.8% | RAM: 189.91GB | GPU: 38.59MB\n",
      "[Batch 113] Temps: 0.00s | CPU: 11.7% | RAM: 189.93GB | GPU: 38.59MB\n",
      "[Batch 114] Temps: 0.00s | CPU: 0.0% | RAM: 189.93GB | GPU: 38.59MB\n",
      "[Batch 115] Temps: 0.00s | CPU: 7.7% | RAM: 189.93GB | GPU: 38.59MB\n",
      "[Batch 116] Temps: 0.00s | CPU: 5.4% | RAM: 189.93GB | GPU: 38.59MB\n",
      "[Batch 117] Temps: 0.00s | CPU: 5.7% | RAM: 189.24GB | GPU: 38.59MB\n",
      "[Batch 118] Temps: 0.00s | CPU: 5.7% | RAM: 189.24GB | GPU: 38.59MB\n",
      "[Batch 119] Temps: 0.00s | CPU: 6.5% | RAM: 189.24GB | GPU: 38.59MB\n",
      "[Batch 120] Temps: 0.00s | CPU: 6.5% | RAM: 189.24GB | GPU: 38.59MB\n",
      "[Batch 121] Temps: 0.00s | CPU: 5.1% | RAM: 189.26GB | GPU: 38.59MB\n",
      "[Batch 122] Temps: 0.00s | CPU: 6.1% | RAM: 189.27GB | GPU: 38.59MB\n",
      "[Batch 123] Temps: 0.00s | CPU: 4.3% | RAM: 189.27GB | GPU: 38.59MB\n",
      "[Batch 124] Temps: 0.00s | CPU: 6.5% | RAM: 189.27GB | GPU: 38.59MB\n",
      "[Batch 125] Temps: 0.00s | CPU: 5.0% | RAM: 189.26GB | GPU: 38.59MB\n",
      "[Batch 126] Temps: 0.00s | CPU: 6.3% | RAM: 189.26GB | GPU: 38.59MB\n",
      "[Batch 127] Temps: 0.00s | CPU: 2.4% | RAM: 189.26GB | GPU: 38.59MB\n",
      "[Batch 128] Temps: 0.00s | CPU: 6.7% | RAM: 189.26GB | GPU: 38.59MB\n",
      "[Batch 129] Temps: 0.00s | CPU: 4.7% | RAM: 189.27GB | GPU: 38.59MB\n",
      "[Batch 130] Temps: 0.00s | CPU: 8.3% | RAM: 189.27GB | GPU: 38.59MB\n",
      "[Batch 131] Temps: 0.00s | CPU: 8.0% | RAM: 189.27GB | GPU: 38.59MB\n",
      "[Batch 132] Temps: 0.00s | CPU: 0.0% | RAM: 189.27GB | GPU: 38.59MB\n",
      "[Batch 133] Temps: 0.00s | CPU: 8.8% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 134] Temps: 0.00s | CPU: 5.5% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 135] Temps: 0.00s | CPU: 3.9% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 136] Temps: 0.00s | CPU: 6.8% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 137] Temps: 0.00s | CPU: 4.4% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 138] Temps: 0.00s | CPU: 3.7% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 139] Temps: 0.00s | CPU: 5.6% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 140] Temps: 0.00s | CPU: 2.1% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 141] Temps: 0.00s | CPU: 3.4% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 142] Temps: 0.00s | CPU: 4.4% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 143] Temps: 0.00s | CPU: 3.4% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 144] Temps: 0.00s | CPU: 4.4% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 145] Temps: 0.00s | CPU: 3.6% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 146] Temps: 0.00s | CPU: 3.9% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 147] Temps: 0.00s | CPU: 6.2% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 148] Temps: 0.00s | CPU: 4.5% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 149] Temps: 0.00s | CPU: 3.5% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 150] Temps: 0.00s | CPU: 3.8% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 151] Temps: 0.00s | CPU: 6.0% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 152] Temps: 0.00s | CPU: 4.4% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 153] Temps: 0.00s | CPU: 3.6% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 154] Temps: 0.00s | CPU: 4.2% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 155] Temps: 0.00s | CPU: 4.2% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 156] Temps: 0.00s | CPU: 4.5% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 157] Temps: 0.00s | CPU: 4.0% | RAM: 189.30GB | GPU: 38.59MB\n",
      "[Batch 158] Temps: 0.00s | CPU: 3.6% | RAM: 189.31GB | GPU: 38.59MB\n",
      "[Batch 159] Temps: 0.00s | CPU: 6.4% | RAM: 189.31GB | GPU: 38.59MB\n",
      "[Batch 160] Temps: 0.00s | CPU: 4.4% | RAM: 189.31GB | GPU: 38.59MB\n",
      "[Batch 161] Temps: 0.00s | CPU: 4.4% | RAM: 189.31GB | GPU: 38.59MB\n",
      "[Batch 162] Temps: 0.00s | CPU: 4.0% | RAM: 189.31GB | GPU: 38.59MB\n",
      "[Batch 163] Temps: 0.00s | CPU: 3.5% | RAM: 189.31GB | GPU: 38.59MB\n",
      "[Batch 164] Temps: 0.00s | CPU: 6.7% | RAM: 189.31GB | GPU: 38.59MB\n",
      "[Batch 165] Temps: 0.00s | CPU: 4.6% | RAM: 189.31GB | GPU: 38.59MB\n",
      "[Batch 166] Temps: 0.00s | CPU: 4.4% | RAM: 189.31GB | GPU: 38.59MB\n",
      "[Batch 167] Temps: 0.00s | CPU: 5.3% | RAM: 189.31GB | GPU: 38.59MB\n",
      "[Batch 168] Temps: 0.00s | CPU: 4.4% | RAM: 189.31GB | GPU: 38.59MB\n",
      "[Batch 169] Temps: 0.00s | CPU: 10.2% | RAM: 189.31GB | GPU: 38.59MB\n",
      "[Batch 170] Temps: 0.00s | CPU: 4.3% | RAM: 189.31GB | GPU: 38.59MB\n",
      "[Batch 171] Temps: 0.00s | CPU: 6.0% | RAM: 189.31GB | GPU: 38.59MB\n",
      "[Batch 172] Temps: 0.00s | CPU: 4.1% | RAM: 189.31GB | GPU: 38.59MB\n",
      "[Batch 173] Temps: 0.00s | CPU: 4.3% | RAM: 189.32GB | GPU: 38.59MB\n",
      "[Batch 174] Temps: 0.00s | CPU: 4.4% | RAM: 189.32GB | GPU: 38.59MB\n",
      "[Batch 175] Temps: 0.00s | CPU: 2.0% | RAM: 189.32GB | GPU: 38.59MB\n",
      "[Batch 176] Temps: 0.00s | CPU: 5.7% | RAM: 189.32GB | GPU: 38.59MB\n",
      "[Batch 177] Temps: 0.00s | CPU: 3.8% | RAM: 189.32GB | GPU: 38.59MB\n",
      "[Batch 178] Temps: 0.00s | CPU: 4.4% | RAM: 189.32GB | GPU: 38.59MB\n",
      "[Batch 179] Temps: 0.00s | CPU: 4.0% | RAM: 189.32GB | GPU: 38.59MB\n",
      "[Batch 180] Temps: 0.00s | CPU: 4.1% | RAM: 189.32GB | GPU: 38.59MB\n",
      "[Batch 181] Temps: 0.00s | CPU: 4.0% | RAM: 189.32GB | GPU: 38.59MB\n",
      "[Batch 182] Temps: 0.00s | CPU: 4.3% | RAM: 189.32GB | GPU: 38.59MB\n",
      "[Batch 183] Temps: 0.00s | CPU: 8.5% | RAM: 189.32GB | GPU: 38.59MB\n",
      "[Batch 184] Temps: 0.00s | CPU: 2.3% | RAM: 189.33GB | GPU: 38.59MB\n",
      "[Batch 185] Temps: 0.00s | CPU: 4.3% | RAM: 189.33GB | GPU: 38.59MB\n",
      "[Batch 186] Temps: 0.00s | CPU: 4.2% | RAM: 189.34GB | GPU: 38.59MB\n",
      "[Batch 187] Temps: 0.00s | CPU: 5.5% | RAM: 189.34GB | GPU: 38.59MB\n",
      "[Batch 188] Temps: 0.00s | CPU: 5.0% | RAM: 189.34GB | GPU: 38.59MB\n",
      "[Batch 189] Temps: 0.00s | CPU: 4.2% | RAM: 189.34GB | GPU: 38.59MB\n",
      "[Batch 190] Temps: 0.00s | CPU: 2.8% | RAM: 189.34GB | GPU: 38.59MB\n",
      "[Batch 191] Temps: 0.00s | CPU: 2.0% | RAM: 189.34GB | GPU: 38.59MB\n",
      "[Batch 192] Temps: 0.00s | CPU: 2.1% | RAM: 189.34GB | GPU: 38.59MB\n",
      "[Batch 193] Temps: 0.04s | CPU: 2.7% | RAM: 189.34GB | GPU: 37.81MB\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "        batch_start = time.time()\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model_deepcnn(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Utilisation CPU/RAM\n",
    "        cpu_usage = psutil.cpu_percent(interval=None)\n",
    "        ram = psutil.virtual_memory()\n",
    "\n",
    "        # Utilisation GPU (si dispo)\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_mem = torch.cuda.memory_allocated(device) / 1024**2  # en MB\n",
    "        else:\n",
    "            gpu_mem = 0.0\n",
    "\n",
    "        print(f\"[Batch {batch_idx+1}] Temps: {time.time()-batch_start:.2f}s | \"\n",
    "              f\"CPU: {cpu_usage:.1f}% | RAM: {ram.used/1024**3:.2f}GB | GPU: {gpu_mem:.2f}MB\")\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temps Test Total: 10.64 sec\n",
      "Throughput: 577.85 images/sec\n"
     ]
    }
   ],
   "source": [
    "total_time = end_time - start_time\n",
    "print(f\"\\nTemps Test Total: {total_time:.2f} sec\")\n",
    "print(f\"Throughput: {len(test_dataset) / total_time:.2f} images/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Set Evaluation (Deep CNN) ===\n",
      "                                               precision    recall  f1-score   support\n",
      "\n",
      "                           Apple___Apple_scab       0.91      0.75      0.82       100\n",
      "                            Apple___Black_rot       0.91      0.97      0.94       100\n",
      "                     Apple___Cedar_apple_rust       0.98      0.92      0.95       100\n",
      "                              Apple___healthy       0.94      0.88      0.91       164\n",
      "                    Background_without_leaves       0.91      0.96      0.94       114\n",
      "                          Blueberry___healthy       0.84      1.00      0.91       150\n",
      "                      Cherry___Powdery_mildew       0.97      0.96      0.97       105\n",
      "                             Cherry___healthy       0.92      0.98      0.95       100\n",
      "   Corn___Cercospora_leaf_spot Gray_leaf_spot       0.90      0.88      0.89       100\n",
      "                           Corn___Common_rust       1.00      0.99      1.00       119\n",
      "                  Corn___Northern_Leaf_Blight       0.91      0.90      0.90       100\n",
      "                               Corn___healthy       1.00      0.97      0.99       116\n",
      "                            Grape___Black_rot       0.90      0.95      0.92       118\n",
      "                 Grape___Esca_(Black_Measles)       0.95      0.96      0.96       138\n",
      "   Grape___Leaf_blight_(Isariopsis_Leaf_Spot)       0.98      0.98      0.98       108\n",
      "                              Grape___healthy       0.97      0.90      0.93       100\n",
      "     Orange___Haunglongbing_(Citrus_greening)       0.99      0.99      0.99       551\n",
      "                       Peach___Bacterial_spot       0.92      0.93      0.92       230\n",
      "                              Peach___healthy       0.90      0.99      0.94       100\n",
      "                Pepper,_bell___Bacterial_spot       0.88      0.90      0.89       100\n",
      "                       Pepper,_bell___healthy       0.93      0.84      0.88       148\n",
      "                        Potato___Early_blight       0.90      0.94      0.92       100\n",
      "                         Potato___Late_blight       0.89      0.84      0.87       100\n",
      "                             Potato___healthy       0.96      0.94      0.95       100\n",
      "                          Raspberry___healthy       0.90      0.99      0.94       100\n",
      "                            Soybean___healthy       0.95      0.98      0.97       509\n",
      "                      Squash___Powdery_mildew       0.99      0.99      0.99       184\n",
      "                     Strawberry___Leaf_scorch       0.98      0.87      0.92       111\n",
      "                         Strawberry___healthy       0.98      0.86      0.91       100\n",
      "                      Tomato___Bacterial_spot       0.95      0.92      0.93       213\n",
      "                        Tomato___Early_blight       0.93      0.62      0.74       100\n",
      "                         Tomato___Late_blight       0.88      0.88      0.88       191\n",
      "                           Tomato___Leaf_Mold       0.91      0.96      0.94       100\n",
      "                  Tomato___Septoria_leaf_spot       0.83      0.92      0.87       177\n",
      "Tomato___Spider_mites Two-spotted_spider_mite       0.86      0.94      0.90       168\n",
      "                         Tomato___Target_Spot       0.84      0.85      0.85       140\n",
      "       Tomato___Tomato_Yellow_Leaf_Curl_Virus       0.98      0.98      0.98       536\n",
      "                 Tomato___Tomato_mosaic_virus       0.96      0.95      0.95       100\n",
      "                             Tomato___healthy       0.99      0.96      0.98       159\n",
      "\n",
      "                                     accuracy                           0.94      6149\n",
      "                                    macro avg       0.93      0.92      0.93      6149\n",
      "                                 weighted avg       0.94      0.94      0.94      6149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rapport complet\n",
    "print(\"=== Test Set Evaluation (Deep CNN) ===\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
